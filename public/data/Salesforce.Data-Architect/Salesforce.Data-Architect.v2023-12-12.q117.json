[
  {
    "content": "A large automobile manufacturer has decided to use Salesforce as its CRM. It needs to maintain the following dealer types in their CRM:\nLocal dealers\nRegional distributor\nState distributor\nService dealer\nThe attributes are different for each of the customer types. The CRM users should be allowed to enter only attributes related to the customer types. The processes and business rules for each of the customer types could be different.\nHow should the different dealers be maintained in Salesforce?",
    "options": [
      "A. Use Accounts for dealers, and create record types for each of the dealer types.",
      "B. Create dealers as Accounts, and build custom views for each of the dealer types.",
      "C. Use Accounts for dealers and custom picklist field for each of the dealer types",
      "D. Create Custom objects for each dealer types and custom fields for dealer attributes."
    ],
    "answer": "A",
    "title": "Question 1",
    "explanation": "According to Trailhead2, record types are a feature that allows administrators to create different business processes, page layouts, and picklist values for different types of records within an object. Record types can be used to maintain different dealer types in Salesforce by creating record types for each of the dealer types and assigning them appropriate attributes, processes, and rules. Option A is the correct answer because it suggests using Accounts for dealers, and creating record types for each of the dealer types. Option B is incorrect because creating dealers as Accounts, and building custom views for each of the dealer types does not allow CRM users to enter only attributes related to the customer types, nor does it enable different processes and business rules for each of the customer types. Option C is incorrect because using Accounts for dealers and custom picklist field for each of the dealer types does not allow CRM users to enter only attributes related to the customer types, nor does it enable different processes and business rules for each of the customer types. Option D is incorrect because creating Custom objects for each dealer types and custom fields for dealer attributes can be unnecessary and complex, as it requires creating multiple objects and relationships instead of using the standard Account object."
  },
  {
    "content": "Universal Containers has received complaints that customers are being called by multiple Sales Reps where the second Sales Rep that calls is unaware of the previous call by their coworker. What is a data quality problem that could cause this?",
    "options": [
      "A. Missing phone number on the Contact record.",
      "B. Customer phone number has changed on the Contact record.",
      "C. Duplicate Contact records exist in the system.",
      "D. Duplicate Activity records on a Contact."
    ],
    "answer": "C",
    "title": "Question 2",
    "explanation": "A data quality problem that could cause customers to be called by multiple Sales Reps is having duplicate Contact records in the system. Duplicate records can result from data entry errors, data imports, or integrations with other systems. Duplicate records can lead to confusion, inefficiency, and customer dissatisfaction"
  },
  {
    "content": "What is an advantage of using Custom metadata type over Custom setting?",
    "options": [
      "A. Custom metadata records are not copied from production to sandbox.",
      "B. Custom metadata types are available for reporting.",
      "C. Custom metadata records are deployable using packages.",
      "D. Custom metadata records are editable in Apex."
    ],
    "answer": "C",
    "title": "Question 3",
    "explanation": "Custom metadata records are deployable using packages, which makes them easier to migrate from one environment to another. Custom settings records are not deployable using packages, and they are copied from production to sandbox. Custom metadata types are not available for reporting, and custom metadata records are not editable in Apex."
  },
  {
    "content": "Cloud Kicks stores Invoice records in a custom object. Invoice records are being sent to the Accounting department with missing States and incorrectly formatted Postal Codes.\nWhich two actions should Cloud Kicks take to improve data quality? (Choose two.)",
    "options": [
      "A. Change each address field to require on the Page Layout.",
      "B. Write an Apex Trigger to require all fields to be populated.",
      "C. Utilize a Validation Rule with a REGEX operator on Postal Code.",
      "D. Utilize a Validation Rule with a CONTAINS operator on address fields."
    ],
    "answer": "C,D",
    "title": "Question 4",
    "explanation": "Utilizing a Validation Rule with a REGEX operator on Postal Code and utilizing a Validation Rule with a CONTAINS operator on address fields are two actions that Cloud Kicks should take to improve data quality for their Invoice records. A Validation Rule with a REGEX operator can check if the Postal Code field matches a specific pattern or format, such as a five-digit number or a combination of letters and numbers. A Validation Rule with a CONTAINS operator can check if the address fields contain certain values, such as valid state abbreviations or country names. These Validation Rules can prevent users from saving invalid or incomplete data and display error messages to guide them to correct the data. The other options are not effective or recommended for improving data quality, as they would either require additional customization, not enforce data standards, or not address the specific issues of missing states and incorrectly formatted postal codes"
  },
  {
    "content": "Universal Containers (UC) is migrating from a legacy system to Salesforce CRM, UC is concerned about the quality of data being entered by users and through external integrations.\nWhich two solutions should a data architect recommend to mitigate data quality issues?",
    "options": [
      "A. Leverage picklist and lookup fields where possible",
      "B. Leverage Apex to validate the format of data being entered via a mobile device.",
      "C. Leverage validation rules and workflows.",
      "D. Leverage third-party- AppExchange tools"
    ],
    "answer": "A,C",
    "title": "Question 5",
    "explanation": "According to the Salesforce documentation1, data quality is the measure of how well the data in Salesforce meets the expectations and requirements of the users and stakeholders. Data quality can be affected by various factors, such as data entry errors, data duplication, data inconsistency, data incompleteness, data timeliness, etc. To mitigate data quality issues, some of the recommended solutions are:\n Leverage picklist and lookup fields where possible (option A). This means using fields that restrict the values or references that can be entered by the users or integrations. This can help reduce data entry errors, enforce data consistency, and improve data accuracy.\n Leverage validation rules and workflows (option C). This means using features that allow defining rules and criteria to validate the data that is entered or updated by the users or integrations. This can help prevent invalid or incorrect data from being saved, and trigger actions or alerts to correct or improve the data.\n Leveraging Apex to validate the format of data being entered via a mobile device (option B) is not a good solution, as it can be complex, costly, and difficult to maintain. It is better to use standard features or declarative tools that can handle data validation more effectively. Leveraging third-party AppExchange tools (option D) is also not a good solution, as it can incur additional costs and dependencies. It is better to use native Salesforce features or custom solutions that can handle data quality more efficiently."
  },
  {
    "content": "The architect is planning a large data migration for Universal Containers from their legacy CRM system to Salesforce. What three things should the architect consider to optimize performance of the data migration? Choose 3 answers",
    "options": [
      "A. Review the time zones of the User loading the data.",
      "B. Remove custom indexes on the data being loaded.",
      "C. Determine if the legacy system is still in use.",
      "D. Defer sharing calculations of the Salesforce Org.",
      "E. Deactivate approval processes and workflow rules."
    ],
    "answer": "B,D,E",
    "title": "Question 6",
    "explanation": "Removing custom indexes on the data being loaded will prevent unnecessary index maintenance and improve the data load speed. Deferring sharing calculations of the Salesforce Org will avoid frequent sharing rule evaluations and reduce the load time. Deactivating approval processes and workflow rules will prevent triggering any automation logic that might slow down or fail the data load."
  },
  {
    "content": "US is implementing salesforce and will be using salesforce to track customer complaints, provide white papers on products and provide subscription (Fee) - based support.\nWhich license type will US users need to fulfil US's requirements?",
    "options": ["A. Lightning platform starter license.", "B. Service cloud license.", "C. Salesforce license.", "D. Sales cloud license"],
    "answer": "B",
    "title": "Question 7",
    "explanation": "The best license type to fulfil US's requirements is the Service Cloud license. Service Cloud licenses are designed for users who need access to customer service features, such as cases, solutions, knowledge articles, entitlements, service contracts, and service console. Service Cloud users can also access standard CRM objects, such as accounts, contacts, leads, opportunities, campaigns, and reports3. Lightning Platform Starter license is not a good option because it is intended for users who need access to one custom app and a limited set of standard objects. Salesforce license is not a specific license type, but rather a generic term for any license that grants access to the Salesforce platform. Sales Cloud license is not a good option because it is intended for users who need access to sales features, such as products, price books, quotes, orders, and forecasts."
  },
  {
    "content": "Universal Containers (UC) is a business that works directly with individual consumers (B2C). They are moving from a current home-grown CRM system to Salesforce. UC has about one million consumer records. What should the architect recommend for optimal use of Salesforce functionality and also to avoid data loading issues?",
    "options": [
      "A. Create a Custom Object Individual Consumer c to load all individual consumers.",
      "B. Load all individual consumers as Account records and avoid using the Contact object.",
      "C. Load one Account record and one Contact record for each individual consumer.",
      "D. Create one Account and load individual consumers as Contacts linked to that one Account."
    ],
    "answer": "D",
    "title": "Question 8",
    "explanation": "According to the exam guide, one of the objectives is to \"describe best practices for implementing a single-org strategy in a B2C scenario\"1. This implies that option D is the best practice for loading individual consumers as contacts in Salesforce. This approach avoids creating unnecessary accounts and reduces data duplication. Option C is not correct because it creates one account per contact, which increases data volume and complexity. Options A and B are not correct because they do not leverage the standard contact object, which provides native functionality and integration with other Salesforce features."
  },
  {
    "content": "Universal Containers (UC) has users complaining about reports timing out or simply taking too long to run What two actions should the data architect recommend to improve the reporting experience? Choose 2 answers",
    "options": [
      "A. Index key fields used in report criteria.",
      "B. Enable Divisions for large data objects.",
      "C. Create one skinny table per report.",
      "D. Share each report with fewer users."
    ],
    "answer": "A,C",
    "title": "Question 9",
    "explanation": "Indexing key fields used in report criteria can speed up the query execution and reduce the report run time. Indexes can be created by Salesforce automatically or manually by request. Creating one skinny table per report can also improve the reporting performance by storing frequently used fields in a separate table that does not include complex formulas or joins."
  },
  {
    "content": "Universal Containers (UC) has a Salesforce instance with over 10.000 Account records. They have noticed similar, but not identical. Account names and addresses. What should UC do to ensure proper data quality?",
    "options": [
      "A. Use a service to standardize Account addresses, then use a 3rd -party tool to merge Accounts based on rules.",
      "B. Run a report, find Accounts whose name starts with the same five characters, then merge those Accounts.",
      "C. Enable Account de -duplication by creating matching rules in Salesforce, which will mass merge duplicate Accounts.",
      "D. Make the Account Owner clean their Accounts' addresses, then merge Accounts with the same address."
    ],
    "answer": "C",
    "title": "Question 10",
    "explanation": "Enabling Account de-duplication by creating matching rules in Salesforce, which will mass merge duplicate Accounts, is what UC should do to ensure proper data quality for their Account records. Matching rules allow UC to define how Salesforce identifies duplicate Accounts based on various criteria, such as name, address, phone number, etc. Mass merge allows UC to merge up to 200 duplicate Accounts at a time, based on the matching rules. This simplifies and automates the process of de-duplicating Accounts and improves data quality. The other options are either more time-consuming, costly, or error-prone for ensuring proper data quality."
  },
  {
    "content": "Universal Container has implemented Sales Cloud to manage patient and related health records. During a recent security audit of the system, it was discovered that same standard and custom fields need to encrypted.\nWhich solution should a data architect recommend to encrypt existing fields?",
    "options": [
      "A. Use Apex Crypto Class encrypt customer and standard fields.",
      "B. Implement classic encryption to encrypt custom and standard fields.",
      "C. Implement shield platform encryption to encrypt and standard fields",
      "D. Expert data out of Salesforce and encrypt custom and standard fields."
    ],
    "answer": "C",
    "title": "Question 11",
    "explanation": "The correct answer is C, implement shield platform encryption to encrypt standard and custom fields. Shield platform encryption is a feature that allows you to encrypt sensitive data at rest in Salesforce without affecting its functionality. You can encrypt both standard and custom fields using shield platform encryption. Using Apex Crypto Class, implementing classic encryption, or exporting data out of Salesforce are not recommended solutions, as they will either limit your functionality, require custom code, or compromise your data security."
  },
  {
    "content": "Universal Containers (UC) has implemented Sales Cloud for its entire sales organization, UC has built a custom object called projects_c that stores customers project detail and employee bitable hours.\nThe following requirements are needed:\nA subnet of individuals from the finance team will need to access to the projects object for reporting and adjusting employee utilization.\nThe finance users will not access to any sales objects, but they will need to interact with the custom object.\nWhich license type a data architect recommend for the finance team that best meets the requirements?",
    "options": ["A. Service Cloud", "B. Sales Cloud", "C. Light Platform Start", "D. Lighting platform plus"],
    "answer": "D",
    "title": "Question 12",
    "explanation": "The data architect should recommend Lighting platform plus license type for the finance team that best meets the requirements. This license type allows users to access custom objects and run reports and dashboards on them, without accessing any sales objects. Option A is incorrect because Service Cloud license type is more suitable for customer service users who need to access cases, knowledge, and other service objects. Option B is incorrect because Sales Cloud license type is more suitable for sales users who need to access accounts, contacts, opportunities, and other sales objects. Option C is incorrect because Light Platform Start license type does not allow users to run reports and dashboards on custom objects."
  },
  {
    "content": "Universal Containers (UC) has a custom discount request object set as a detail object with a custom product object as the master. There is a requirement to allow the creation of generic discount requests without the custom product object as its master record. What solution should an Architect recommend to UC?",
    "options": [
      "A. Mandate the selection of a custom product for each discount request.",
      "B. Create a placeholder product record for the generic discount request.",
      "C. Remove the master-detail relationship and keep the objects separate.",
      "D. Change the master-detail relationship to a lookup relationship."
    ],
    "answer": "D",
    "title": "Question 13",
    "explanation": "Changing the master-detail relationship to a lookup relationship is the best solution for allowing the creation of generic discount requests without the custom product object as its master record. A lookup relationship allows you to create child records without requiring a parent record. It also gives you more flexibility in defining the sharing and security settings for each object"
  },
  {
    "content": "Based on government regulations, a Salesforce customer plans to implement the following in Salesforce for compliance:\nAccess to customer information based on record ownership\nAbility for customers to request removal of their information from Salesforce Prevent users from accessing Salesforce from outside company network (virtual private network, or VPN) What should a data architect recommend to address these requirements?",
    "options": [
      "A. Contact Salesforce support to restrict access only with VPN and other requirements",
      "B. Allow users access to Salesforce through a custom web application hosted within VPN.",
      "C. Implement IP restrictions, sharing settings, and custom Apex to support customer requests.",
      "D. Implement Salesforce shield with Event Monitoring to address the requirement."
    ],
    "answer": "C",
    "title": "Question 14",
    "explanation": "IP restrictions can be used to prevent users from accessing Salesforce from outside the company network or VPN. According to this article, sharing settings can be used to control access to customer information based on record ownership. According to this article, custom Apex can be used to support customer requests for removal of their information from Salesforce."
  },
  {
    "content": "UC is migrating individual customers (B2C) data from legacy systems to SF. There are millions of customers stored as accounts and contacts in legacy database.\nWhich object model should a data architect configure within SF?",
    "options": [
      "A. Leverage person account object in Salesforce",
      "B. Leverage custom person account object in SF",
      "C. Leverage custom account and contact object in SF",
      "D. Leverage standard account and contact object in SF"
    ],
    "answer": "A",
    "title": "Question 15",
    "explanation": "The best object model to configure within SF for migrating individual customers (B2C) data from legacy systems is to leverage person account object in Salesforce. Person accounts are a special type of accounts that store information about individual people by combining certain account and contact fields into a single record4. Person accounts are useful for B2C scenarios where there is no need to associate a company name with a contact. Person accounts also support standard Salesforce features and functionality, such as leads, campaigns, reports, dashboards, etc5. Leverage custom person account object in SF is not a good option because there is no such thing as a custom person account object. Leverage custom account and contact object in SF is not a good option because it would require creating and maintaining additional objects and fields that may not be necessary or compatible with standard Salesforce features. Leverage standard account and contact object in SF is not a good option because it would require filling in dummy values for the account name field, which is mandatory for standard accounts."
  },
  {
    "content": "NTO has been using salesforce for sales and service for 10 years. For the past 2 years, the marketing group has noticed a raise from 0 to 35 % in returned mail when sending mail using the contact information stored in salesforce.\nWhich solution should the data architect use to reduce the amount of returned mails?",
    "options": [
      "A. Use a 3rd-party data source to update contact information in salesforce.",
      "B. Email all customer and asked them to verify their information and to call NTO if their address is incorrect.",
      "C. Delete contacts when the mail is returned to save postal cost to NTO.",
      "D. Have the sales team to call all existing customers and ask to verify the contact details."
    ],
    "answer": "A",
    "title": "Question 16",
    "explanation": "Using a third-party data source to update contact information in Salesforce is the best solution to reduce the amount of returned mails. This way, the data architect can ensure that the contact information is accurate and up-to-date without relying on manual verification or deletion of contacts"
  },
  {
    "content": "Get Cloudy Consulting monitors 15,000 servers, and these servers automatically record their status every 10 minutes. Because of company policy, these status reports must be maintained for 5 years. Managers at Get Cloudy Consulting need access to up to one week's worth of these status reports with all of their details.\nAn Architect is recommending what data should be integrated into Salesforce and for how long it should be stored in Salesforce.\nWhich two limits should the Architect be aware of? (Choose two.)",
    "options": ["A. Data storage limits", "B. Workflow rule limits", "C. API Request limits", "D. Webservice callout limits"],
    "answer": "A,C",
    "title": "Question 17",
    "explanation": "Data storage limits and API request limits are two important factors that affect the data integration and storage in Salesforce. Data storage limits determine how much data can be stored in Salesforce, and API request limits determine how many API calls can be made to Salesforce in a 24-hour period. Both of these limits depend on the edition and license type of the Salesforce org. Workflow rule limits and webservice callout limits are not directly related to data integration and storage, but rather to business logic and external services."
  },
  {
    "content": "Universal Containers (UC) is migrating from an on-premise homegrown customer relationship management (CRM) system- During analysis, UC users highlight a pain point that there are multiple versions of many customers.\nWhat should the data architect do for a successful migration to mitigate the pain point?",
    "options": [
      "A. Hire an intern manually de-duplicate the records after migrating to Salesforce.",
      "B. Migrate the data as is, and use Salesforce's de-duplicating feature.",
      "C. Have the users manually clean the data in the old system prior to migration.",
      "D. Store the data in a staging database, and de-duplicate identical records."
    ],
    "answer": "D",
    "title": "Question 18",
    "explanation": "Storing the data in a staging database and de-duplicating identical records (option D) is the best solution for a successful migration to mitigate the pain point, as it allows the data architect to identify and merge duplicate customers before they are imported into Salesforce. Hiring an intern manually de-duplicate the records after migrating to Salesforce (option A) is not a good solution, as it may be time-consuming and error-prone, and it does not prevent duplicate records from being created in Salesforce. Migrating the data as is and using Salesforce's de-duplicating feature (option B) is also not a good solution, as it may cause data quality issues and conflicts, and it does not address the root cause of the duplication. Having the users manually clean the data in the old system prior to migration (option C) is also not a good solution, as it may be unrealistic and impractical, and it does not leverage any automated tools or processes."
  },
  {
    "content": "What makes Skinny tables fast? Choose three answers.",
    "options": [
      "A. They do not include soft-deleted records",
      "B. They avoid resource intensive joins",
      "C. Their tables are kept in sync with their source tables when the source tables are modified",
      "D. They can contain fields from other objects",
      "E. They support up to a max of 100 of columns"
    ],
    "answer": "A,B,C",
    "title": "Question 19",
    "explanation": "Skinny tables are custom tables that contain frequently used fields from a standard or custom object. They are used to improve performance by reducing the number of database joins required for queries. Skinny tables have the following characteristics1:\n They do not include soft-deleted records, which means they only contain active records and save space.\n They avoid resource intensive joins by storing data from multiple objects in one table, which reduces the query time and complexity.\n Their tables are kept in sync with their source tables when the source tables are modified, which ensures data consistency and accuracy."
  },
  {
    "content": "Universal Containers (UC) is in the process of selling half of its company. As part of this split, UC's main Salesforce org will be divided into two org:org A and org B, UC has delivered these requirements to its data architect\n1. The data model for Org B will drastically change with different objects, fields, and picklist values.\n2. Three million records will need to be migrated from org A to org B for compliance reasons.\n3. The migrate will need occur within the next two month, prior to be split.\nWhich migrate strategy should a data architect use to successfully migrate the date?",
    "options": [
      "A. use as ETL tool to orchestrate the migration.",
      "B. Use Data Loader for export and Data Import Wizard for import",
      "C. Write a script to use the Bulk API",
      "D. Use the Salesforces CLI to query, export, and import"
    ],
    "answer": "A",
    "title": "Question 20",
    "explanation": "Using an ETL tool to orchestrate the migration is the best strategy for this scenario, as it can handle the data model changes, the large volume of records, and the tight timeline. Writing a script to use the Bulk API (option C) is also possible, but it would require more coding and testing effort. Using Data Loader and Data Import Wizard (option B) is not suitable for migrating three million records, as they have limitations on the batch size and the number of records per operation. Using Salesforce CLI (option D) is also not recommended for large data migration, as it is mainly designed for development and testing purposes"
  },
  {
    "content": "Universal Containers (UC) maintains a collection of several million Account records that represent business in the United Sates. As a logistics company, this list is one of the most valuable and important components of UC's business, and the accuracy of shipping addresses is paramount. Recently it has been noticed that too many of the addresses of these businesses are inaccurate, or the businesses don't exist. Which two scalable strategies should UC consider to improve the quality of their Account addresses?",
    "options": [
      "A. Contact each business on the list and ask them to review and update their address information.",
      "B. Build a team of employees that validate Accounts by searching the web and making phone calls.",
      "C. Integrate with a third-party database or services for address validation and enrichment.",
      "D. Leverage Data.com Clean to clean up Account address fields with the D&B database."
    ],
    "answer": "C,D",
    "title": "Question 21",
    "explanation": "Integrating with a third-party database or service for address validation and enrichment is a scalable strategy that can improve the quality of the Account addresses by comparing them with a reliable source of data1. Leveraging Data.com Clean to clean up Account address fields with the D&B database is another scalable strategy that can automatically update and enrich Account records with verified information from Data.com2."
  },
  {
    "content": "The head of sales at Get Cloudy Consulting wants to understand key relevant performance figures and help managers take corrective actions where appropriate.\nWhat is one reporting option Get Cloudy Consulting should consider?",
    "options": ["A. Case SLA performance report", "B. Sales KPI Dashboard", "C. Opportunity analytic snapshot", "D. Lead conversion rate report"],
    "answer": "B",
    "title": "Question 22",
    "explanation": "A Sales KPI Dashboard is one reporting option that Get Cloudy Consulting should consider to understand key relevant performance figures and help managers take corrective actions where appropriate. A Sales KPI Dashboard can display various metrics that indicate the health and effectiveness of the sales team, such as quota attainment, pipeline value, win rate, average deal size, sales cycle length, and more. A Sales KPI Dashboard can also help identify trends, patterns, and areas for improvement."
  },
  {
    "content": "Universal Containers (UC) is facing data quality issues where Sales Reps are creating duplicate customer accounts, contacts, and leads. UC wants to fix this issue immediately by prompting users about a record that possibly exists in Salesforce. UC wants a report regarding duplicate records. What would be the recommended approach to help UC start immediately?",
    "options": [
      "A. Create an after insert and update trigger on the account, contact and lead, and send an error if a duplicate is found using a custom matching criteria.",
      "B. Create a duplicate rule for account, lead, and contact, use standard matching rules for these objects, and set the action to report and alert for both creates and edits.",
      "C. Create a duplicate rule for account, lead, and contact, use standard matching rules for these objects, and set the action to block for both creates and edits.",
      "D. Create a before insert and update trigger on account, contact, and lead, and send an error if a duplicate is found using a custom matching criteria."
    ],
    "answer": "B",
    "title": "Question 23",
    "explanation": "Creating a duplicate rule for account, lead, and contact, using standard matching rules for these objects, and setting the action to report and alert for both creates and edits can help UC fix the issue immediately by prompting users about a record that possibly exists in Salesforce. This can also generate a report regarding duplicate records that can be used for further analysis and resolution"
  },
  {
    "content": "Universal Containers (CU) is in the process of implementing an enterprise data warehouse (EDW). UC needs to extract 100 million records from Salesforce for migration to the EDW.\nWhat data extraction strategy should a data architect use for maximum performance?",
    "options": [
      "A. Install a third-party AppExchange tool.",
      "B. Call the REST API in successive queries.",
      "C. Utilize PK Chunking with the Bulk API.",
      "D. Use the Bulk API in parallel mode."
    ],
    "answer": "C",
    "title": "Question 24",
    "explanation": "According to the Salesforce documentation2, extracting large amounts of data from Salesforce can be challenging and time-consuming, as it can encounter performance issues, API limits, timeouts, etc. To extract 100 million records from Salesforce for migration to an enterprise data warehouse (EDW), a data extraction strategy that can provide maximum performance is:\n Utilize PK Chunking with the Bulk API (option C). This means using a feature that allows splitting a large query into smaller batches based on the record IDs (primary keys) of the queried object. This can improve performance and avoid timeouts by processing each batch asynchronously and in parallel using the Bulk API3.\n Installing a third-party AppExchange tool (option A) is not a good solution, as it can incur additional costs and dependencies. It may also not be able to handle such a large volume of data efficiently. Calling the REST API in successive queries (option B) is also not a good solution, as it can encounter API limits and performance issues when querying such a large volume of data. Using the Bulk API in parallel mode (option D) is also not a good solution, as it can still cause timeouts and errors when querying such a large volume of data without chunking."
  },
  { "content": "", "options": [], "answer": "", "title": "Question ", "explanation": "" },
  {
    "content": "Northern Trail Outfitters needs to implement an archive solution for Salesforce dat a. This archive solution needs to help NTO do the following:\n1. Remove outdated Information not required on a day-to-day basis.\n2. Improve Salesforce performance.\nWhich solution should be used to meet these requirements?",
    "options": [
      "A. Identify a location to store archived data and use scheduled batch jobs to migrate and purge the aged data on a nightly basis,",
      "B. Identify a location to store archived data, and move data to the location using a time-based workflow.",
      "C. Use a formula field that shows true when a record reaches a defined age and use that field to run a report and export a report into SharePoint.",
      "D. Create a full copy sandbox, and use it as a source for retaining archived data."
    ],
    "answer": "A",
    "title": "Question 26",
    "explanation": "Identifying a location to store archived data and using scheduled batch jobs to migrate and purge the aged data on a nightly basis can be a way to meet the requirements for an archive solution. The article provides a use case of how to use Heroku Connect, Postgres, and Salesforce Connect to archive old data, free up space in the org, and still retain the option to unarchive the data if needed. The article also explains how this solution can improve Salesforce performance and meet data retention policies."
  },
  {
    "content": "Northern Trail Outfitters (NTO) wants to implement backup and restore for Salesforce data, Currently, it has data backup processes that runs weekly, which back up all Salesforce data to an enterprise data warehouse (EDW). NTO wants to move to daily backups and provide restore capability to avoid any data loss in case of outage.\nWhat should a data architect recommend for a daily backup and restore solution?",
    "options": [
      "A. Use AppExchange package for backup and restore.",
      "B. Use ETL for backup and restore from EDW.",
      "C. Use Bulk API to extract data on daily basis to EDW and REST API for restore.",
      "D. Change weekly backup process to daily backup, and implement a custom restore solution."
    ],
    "answer": "A",
    "title": "Question 27",
    "explanation": "The data architect should recommend using AppExchange package for backup and restore. AppExchange is a marketplace for Salesforce apps and solutions that can be installed and configured in Salesforce orgs. There are several AppExchange packages that provide backup and restore functionality for Salesforce data, such as OwnBackup, Odaseva, or Spanning. These packages can perform daily backups of Salesforce data to a secure cloud storage, and provide restore capability to avoid any data loss in case of outage. Option B is incorrect because using ETL (Extract, Transform, Load) for backup and restore from EDW (Enterprise Data Warehouse) will require additional development effort and may not be reliable or secure. Option C is incorrect because using Bulk API to extract data on daily basis to EDW and REST API for restore will require additional integration effort and may not be scalable or performant. Option D is incorrect because changing weekly backup process to daily backup, and implementing a custom restore solution will require additional configuration effort and may not be robust or compliant."
  },
  {
    "content": "Universal Containers is looking to use Salesforce to manage their sales organization. They will be migrating legacy account data from two aging systems into Salesforce. Which two design considerations should an architect take to minimize data duplication? Choose 2 answers",
    "options": [
      "A. Use a workflow to check and prevent duplicates.",
      "B. Clean data before importing to Salesforce.",
      "C. Use Salesforce matching and duplicate rules.",
      "D. Import the data concurrently."
    ],
    "answer": "B,C",
    "title": "Question 28",
    "explanation": "Cleaning data before importing to Salesforce and using Salesforce matching and duplicate rules are two design considerations that an architect should take to minimize data duplication when migrating legacy account data from two aging systems into Salesforce. Cleaning data before importing involves removing or correcting any inaccurate, incomplete, or inconsistent data from the source systems, as well as identifying and resolving any potential duplicates. This ensures that only high-quality and unique data is imported to Salesforce. Using Salesforce matching and duplicate rules allows the architect to define how Salesforce identifies duplicate records during import and how users can handle them. This prevents or reduces the creation of duplicate records in Salesforce and improves data quality. The other options are not effective or recommended for minimizing data duplication."
  },
  {
    "content": "NTO would like to retrieve their SF orgs meta data programmatically for backup within a various external. Which API is the best fit for accomplishing this task?",
    "options": ["A. Metadata API", "B. Tooling API", "C. Bulk API in serial mode", "D. SOAP API"],
    "answer": "A",
    "title": "Question 29",
    "explanation": "The best API for retrieving Salesforce org metadata programmatically is the Metadata API. The Metadata API provides access to the metadata that defines the structure and configuration of an org, such as custom objects, fields, workflows, security settings, etc. It also supports deploying, retrieving, creating, updating, and deleting metadata components. The Metadata API can be used with various tools, such as Ant, Workbench, or IDEs."
  },
  {
    "content": "A large telecommunication provider that provides internet services to both residence and business has the following attributes:\nA customer who purchases its services for their home will be created as an Account in Salesforce.\nIndividuals within the same house address will be created as Contact in Salesforce.\nBusinesses are created as Accounts in Salesforce.\nSome of the customers have both services at their home and business.\nWhat should a data architect recommend for a single view of these customers without creating multiple customer records?",
    "options": [
      "A. Customers are created as Contacts and related to Business and Residential Accounts using the Account Contact Relationships.",
      "B. Customers are created as Person Accounts and related to Business and Residential Accounts using the Account Contact relationship.",
      "C. Customer are created as individual objects and relate with Accounts for Business and Residence accounts.",
      "D. Costumers are created as Accounts for Residence Account and use Parent Account to relate Business Account."
    ],
    "answer": "A",
    "title": "Question 30",
    "explanation": "Creating customers as Contacts and relating them to Business and Residential Accounts using the Account Contact Relationships (option A) is the best option to recommend for a single view of these customers without creating multiple customer records, as it allows the data architect to model complex relationships between customers and accounts using native Salesforce features and tools. Creating customers as Person Accounts and relating them to Business and Residential Accounts using the Account Contact relationship (option B) is not a good option, as it may create data redundancy and inconsistency, and it does not leverage the existing Contact object. Creating customers as individual objects and relating them with Accounts for Business and Residence accounts (option C) is also not a good option, as it may require more customization and maintenance effort, and it does not leverage the existing Account and Contact objects. Creating customers as Accounts for Residence Account and using Parent Account to relate Business Account (option D) is also not a good option, as it may create confusion and complexity with the account hierarchy, and it does not leverage the existing Contact object."
  },
  {
    "content": "UC has a classic encryption for Custom fields and is leveraging weekly data reports for data backups. During the data validation of exported data UC discovered that encrypted field values are still being exported as part of data exported. What should a data architect recommend to make sure decrypted values are exported during data export?",
    "options": [
      "A. Set a standard profile for Data Migration user, and assign view encrypted data",
      "B. Create another field to copy data from encrypted field and use this field in export",
      "C. Leverage Apex class to decrypt data before exporting it.",
      "D. Set up a custom profile for data migration user and assign view encrypted data."
    ],
    "answer": "B",
    "title": "Question 31",
    "explanation": "The best solution to make sure decrypted values are exported during data export is to create another field to copy data from encrypted field and use this field in export. This is because classic encryption does not support exporting decrypted values of encrypted fields. The view encrypted data permission only allows users to view decrypted values in the user interface, but not in reports or data exports. Therefore, a workaround is to create a formula field or a workflow field update that copies the value of the encrypted field to another field, and use that field for data export. However, this solution has some drawbacks, such as exposing sensitive data in plain text and consuming extra storage space. A better solution would be to use Shield Platform Encryption, which supports exporting decrypted values of encrypted fields with the Export Encrypted Data permission"
  },
  {
    "content": "Universal Containers (UC) has implemented a master data management strategy, which uses a central system of truth, to ensure the entire company has the same customer information in all systems. UC customer data changes need to be accurate at all times in all of the systems. Salesforce is the identified system of record for this information.\nWhat is the correct solution for ensuring all systems using customer data are kept up to date?",
    "options": [
      "A. Send customer data nightly to the system of truth in a scheduled batch job.",
      "B. Send customer record changes from Salesforce to each system in a nightly batch job.",
      "C. Send customer record changes from Salesforce to the system of truth in real time.",
      "D. Have each system pull the record changes from Salesforce using change data capture."
    ],
    "answer": "D",
    "title": "Question 32",
    "explanation": "Having each system pull the record changes from Salesforce using change data capture (option D) is the correct solution for ensuring all systems using customer data are kept up to date, as it allows the systems to subscribe to real-time events from Salesforce and receive notifications when customer records are created, updated, deleted, or undeleted. Sending customer data nightly to the system of truth in a scheduled batch job (option A) or sending customer record changes from Salesforce to each system in a nightly batch job (option B) are not good solutions, as they may cause data latency and inconsistency, and they do not provide real-time updates. Sending customer record changes from Salesforce to the system of truth in real time (option C) is also not a good solution, as it does not address how the other systems will receive the updates from the system of truth."
  },
  {
    "content": "Universal containers is implementing Salesforce lead management. UC Procure lead data from multiple sources and would like to make sure lead data as company profile and location information. Which solution should a data architect recommend to make sure lead data has both profile and location information? Option",
    "options": [
      "A. Ask sales people to search for populating company profile and location data",
      "B. Run reports to identify records which does not have company profile and location data",
      "C. Leverage external data providers populate company profile and location data",
      "D. Export data out of Salesforce and send to another team to populate company profile and location data"
    ],
    "answer": "C",
    "title": "Question 33",
    "explanation": "The best solution to make sure lead data has both profile and location information is to leverage external data providers to populate company profile and location data. This is because external data providers can enrich lead data with additional information from third-party sources, such as Dun & Bradstreet, ZoomInfo, or Clearbit. This can help improve lead quality, segmentation, and conversion. Salesforce supports integrating with external data providers using Data.com Clean or other AppExchange solutions2. Asking sales people to search for populating company profile and location data is inefficient and prone to errors. Running reports to identify records which do not have company profile and location data is useful, but does not solve the problem of how to populate the missing data. Exporting data out of Salesforce and sending to another team to populate company profile and location data is cumbersome and time-consuming."
  },
  {
    "content": "A health care provider wishes to use salesforce to track patient care. The following actions are in Salesforce\n1. Payment Providers: Orgas who pay for the care 2 patients.\n2. Doctors: They provide care plan for patients and need to support multiple patients, they are provided access to patient information.\n3. Patients: They are individuals who need care.\nA data architect needs to map the actor to Sf objects. What should be the optimal selection by the data architect?",
    "options": [
      "A. Patients as Contacts, Payment providers as Accounts, & Doctors as Accounts",
      "B. Patients as Person Accounts, Payment providers as Accounts, & Doctors as Contacts",
      "C. Patients as Person Accounts, Payment providers as Accounts, & Doctors as Person Account",
      "D. Patients as Accounts, Payment providers as Accounts, & Doctors as Person Accounts"
    ],
    "answer": "C",
    "title": "Question 34",
    "explanation": "Patients as Person Accounts, Payment providers as Accounts, & Doctors as Person Accounts is the optimal selection by the data architect to map the actor to Salesforce objects. This is because Person Accounts are a special type of accounts that can store both business and personal information for individual customers. Payment providers are organizations that pay for the care of patients, so they can be modeled as Accounts. Doctors are also individuals who provide care plans for patients and need access to patient information, so they can also be modeled as Person Accounts."
  },
  {
    "content": "The data architect for UC has written a SOQL query that will return all records from the Task object that do not have a value in the WhatID field:\nSelect id, description, Subject from Task where WhatId!= NULL\nWhen the data architect usages the query to select values for a process a time out error occurs.\nWhat does the data architect need to change to make this query more performant?",
    "options": [
      "A. Remove description from the requested field set.",
      "B. Change query to SOSL.",
      "C. Add limit 100 to the query.",
      "D. Change the where clause to filter by a deterministic defined value."
    ],
    "answer": "D",
    "title": "Question 35",
    "explanation": "According to the Salesforce documentation, SOQL is a query language that allows querying data from Salesforce objects and fields. SOQL queries have various clauses and operators that can be used to filter and sort the results. However, some clauses and operators can affect the performance of SOQL queries by increasing the cost or complexity of executing them.\n To make this query more performant, a data architect should change the where clause to filter by a deterministic defined value (option D). This means using a filter condition that specifies a concrete value or range of values for a field, such as WhatId = '001xx000003DGg3' or WhatId IN ('001xx000003DGg3', '001xx000003DGg4'). This can improve the performance of the query by reducing the number of records that need to be scanned and returned. A deterministic defined value can also leverage an index on the field, which can speed up the query execution.\n Removing description from the requested field set (option A) is not a good solution, as it can affect the functionality or usability of the query. The description field may contain important or relevant information that is needed for the process. Changing the query to SOSL (option B) is also not a good solution, as SOSL is a different query language that allows searching text fields across multiple objects. SOSL queries have different syntax and limitations than SOQL queries, and may not return the same results or performance. Adding limit 100 to the query (option C) is also not a good solution, as it can affect the completeness or accuracy of the query. The limit clause specifies the maximum number of records that can be returned by the query, which may not include all the records that match the filter condition."
  },
  {
    "content": "Universal Containers (UC) is implementing its new Internet of Things technology, which consists of smart containers that provide information on container temperature and humidity updated every 10 minutes back to UC. There are roughly 10,000 containers equipped with this technology with the number expected to increase to 50,000 across the next five years. It is essential that Salesforce user have access to current and historical temperature and humidity data for each container. What is the recommended solution?",
    "options": [
      "A. Create new custom fields for temperature and humidity in the existing Container custom object, as well as an external ID field that is unique for each container. These custom fields are updated when a new measure is received.",
      "B. Create a new Container Reading custom object, which is created when a new measure is received for a specific container. The Container Reading custom object has a master-detail relationship to the container object.",
      "C. Create a new Lightning Component that displays last humidity and temperature data for a specific container and can also display historical trends obtaining relevant data from UC's existing data warehouse.",
      "D. Create a new Container Reading custom object with a master-detail relationship to Container which is created when a new measure is received for a specific container. Implement an archiving process that runs every hour."
    ],
    "answer": "D",
    "title": "Question 36",
    "explanation": "The recommended solution for Universal Containers (UC) to implement its new Internet of Things technology is to create a new Container Reading custom object with a master-detail relationship to Container which is created when a new measure is received for a specific container. Implement an archiving process that runs every hour. This solution would allow UC to store and access current and historical temperature and humidity data for each container on Salesforce, and use reports and dashboards to analyze it. However, since UC expects a large volume of data over time, they should implement an archiving process that moves data off-platform after a certain period of time to avoid hitting the Org data storage limit and maintain optimal performance. The other options are not recommended, as they would either not store the historical data on Salesforce, or create too many custom fields on the Container object that could impact performance and usability."
  },
  {
    "content": "Universal Containers (UC) is implementing a formal, cross -business -unit data governance program As part of the program, UC will implement a team to make decisions on enterprise -wide data governance. Which two roles are appropriate as members of this team? Choose 2 answers",
    "options": ["A. Analytics/BI Owners", "B. Data Domain Stewards", "C. Salesforce Administrators", "D. Operational Data Users"],
    "answer": "A,B",
    "title": "Question 37",
    "explanation": "Analytics/BI Owners and Data Domain Stewards are appropriate roles as members of a team that makes decisions on enterprise-wide data governance. Analytics/BI Owners are responsible for defining the business requirements and metrics for data analysis and reporting, and Data Domain Stewards are responsible for defining and enforcing the data quality standards and rules for specific data domains. Salesforce Administrators and Operational Data Users are not suitable roles for this team, as they are more focused on the operational aspects of data management, such as configuration, maintenance, and usage."
  },
  {
    "content": "Universal Containers has a public website with several forms that create Lead records in Salesforce using the REST API. When designing these forms, which two techniques will help maintain a high level of data quality?",
    "options": [
      "A. Do client-side validation of phone number and email field formats.",
      "B. Prefer picklist form fields over free text fields, where possible.",
      "C. Ensure the website visitor is browsing using an HTTPS connection.",
      "D. Use cookies to track when visitors submit multiple forms."
    ],
    "answer": "A,B",
    "title": "Question 38",
    "explanation": "Doing client-side validation of phone number and email field formats and preferring picklist form fields over free text fields, where possible, are two techniques that will help maintain a high level of data quality when designing forms that create Lead records in Salesforce using the REST API. Client-side validation can ensure that the input data is in the correct format and prevent errors or invalid values from being submitted. Picklist form fields can provide a predefined set of values for the users to choose from and avoid typos or inconsistencies in the data."
  },
  {
    "content": "Universal Containers is integrating a new Opportunity engagement system with Salesforce. According to their Master Data Management strategy, Salesforce is the system of record for Account, Contact, and Opportunity dat a. However, there does seem to be valuable Opportunity data in the new system that potentially conflicts with what is stored in Salesforce. What is the recommended course of action to appropriately integrate this new system?",
    "options": [
      "A. The MDM strategy defines Salesforce as the system of record, so Salesforce Opportunity values prevail in all conflicts.",
      "B. A policy should be adopted so that the system whose record was most recently updated should prevail in conflicts.",
      "C. The Opportunity engagement system should become the system of record for Opportunity records.",
      "D. Stakeholders should be brought together to discuss the appropriate data strategy moving forward."
    ],
    "answer": "D",
    "title": "Question 39",
    "explanation": "The recommended course of action to appropriately integrate the new Opportunity engagement system with Salesforce is to bring the stakeholders together to discuss the appropriate data strategy moving forward. This is because there may be valuable data in both systems that need to be reconciled and harmonized, and the Master Data Management (MDM) strategy may need to be revised or updated to accommodate the new system. The other options are not recommended, as they may result in data loss, inconsistency, or duplication"
  },
  {
    "content": "Universal Containers is exporting 40 million Account records from Salesforce using Informatica Cloud. The ETL tool fails and the query log indicates a full table scan time-out failure. What is the recommended solution?",
    "options": [
      "A. Modify the export job header to specify Export-in-Parallel.",
      "B. Modify the export job header to specify Sforce-Enable-PKChunking.",
      "C. Modify the export query that includes standard index fields(s).",
      "D. Modify the export query with LIMIT clause with Batch size 10,000."
    ],
    "answer": "B",
    "title": "Question 40",
    "explanation": "The Sforce-Enable-PKChunking header enables you to extract large data sets from Salesforce by using a technique called primary key (PK) chunking. PK chunking splits your query into multiple queries based on the record IDs of the queried object. This reduces the query time and avoids the query timeout issues that can occur with large data volumes"
  },
  {
    "content": "As part of a phased Salesforce rollout. there will be 3 deployments spread out over the year. The requirements have been carefully documented. Which two methods should an architect use to trace back configuration changes to the detailed requirements? Choose 2 answers",
    "options": [
      "A. Maintain a data dictionary with the justification for each field.",
      "B. Review the setup audit trail for configuration changes.",
      "C. Put the business purpose in the Description of each field.",
      "D. Use the Force.com IDE to save the metadata files in source control."
    ],
    "answer": "C,D",
    "title": "Question 41",
    "explanation": "Option B is correct because putting the business purpose in the Description of each field is a method that an architect can use to trace back configuration changes to the detailed requirements1. The Description of each field provides a brief explanation of what the field is used for and why it is needed2. Option D is correct because using the Force.com IDE to save the metadata files in source control is another method that an architect can use to trace back configuration changes to the detailed requirements1. The Force.com IDE is an integrated development environment that allows developers to work with Salesforce metadata files and Apex code3. Source control is a system that tracks and manages changes to code and configuration files4. Option A is not correct because reviewing the setup audit trail for configuration changes is not a method to trace back configuration changes to the detailed requirements, but a way to monitor and audit the changes made in the setup area. Option C is not correct because maintaining a data dictionary with the justification for each field is not a method to trace back configuration changes to the detailed requirements, but a document that provides information about the data entities and attributes in a system."
  },
  {
    "content": "Universal Containers (UC) wants to store product data in Salesforce, but the standard Product object does not support the more complex hierarchical structure which is currently being used in the product master system. How can UC modify the standard Product object model to support a hierarchical data structure in order to synchronize product data from the source system to Salesforce?",
    "options": [
      "A. Create a custom lookup filed on the standard Product to reference the child record in the hierarchy.",
      "B. Create a custom lookup field on the standard Product to reference the parent record in the hierarchy.",
      "C. Create a custom master-detail field on the standard Product to reference the child record in the hierarchy.",
      "D. Create an Apex trigger to synchronize the Product Family standard picklist field on the Product object."
    ],
    "answer": "B",
    "title": "Question 42",
    "explanation": "Creating a custom lookup field on the standard Product to reference the parent record in the hierarchy is the correct way to modify the standard Product object model to support a hierarchical data structure. This allows UC to create a self-relationship on the Product object and define parent-child relationships among products."
  },
  {
    "content": "Universal Containers (UC) management has identified a total of ten text fields on the Contact object as important to capture any changes made to these fields, such as who made the change, when they made the change, what is the old value, and what is the new value. UC needs to be able to report on these field data changes within Salesforce for the past 3 months. What are two approaches that will meet this requirement? Choose 2 answers",
    "options": [
      "A. Create a workflow to evaluate the rule when a record is created and use field update actions to store previous values for these ten fields in ten new fields.",
      "B. Write an Apex trigger on Contact after insert event and after update events and store the old values in another custom object.",
      "C. Turn on field Contact object history tracking for these ten fields, then create reports on contact history.",
      "D. Create a Contact report including these ten fields and Salesforce Id, then schedule the report to run once a day and send email to the admin."
    ],
    "answer": "B,C",
    "title": "Question 43",
    "explanation": "To capture and report on any changes made to ten text fields on the Contact object for the past 3 months, the data architect should write an Apex trigger on Contact after insert and after update events and store the old values in another custom object, or turn on field Contact object history tracking for these ten fields and create reports on contact history. An Apex trigger can capture the old and new values of the fields, as well as the user and time of the change, and store them in a custom object that can be used for reporting. Field history tracking can also track the changes to the fields and store them in a history table that can be used for reporting. However, field history tracking only retains data for up to 18 months or 24 months with an extension, so it may not be suitable for longer-term reporting needs. The other options are not feasible or effective for capturing and reporting on field data changes."
  },
  {
    "content": "UC is using SF CRM. UC sales managers are complaining about data quality and would like to monitor and measure data quality.\nWhich 2 solutions should a data architect recommend to monitor and measure data quality?\nChoose 2 answers.",
    "options": [
      "A. Use custom objects and fields to identify issues.",
      "B. Review data quality reports and dashboards.",
      "C. Install and run data quality analysis dashboard app",
      "D. Export data and check for data completeness outside of Salesforce."
    ],
    "answer": "B,C",
    "title": "Question 44",
    "explanation": "Reviewing data quality reports and dashboards and installing and running data quality analysis dashboard app are two solutions that can help monitor and measure data quality. Data quality reports and dashboards can provide insights into the completeness, accuracy, and consistency of the data. Data quality analysis dashboard app is a free app from AppExchange that can help analyze and improve data quality by identifying duplicate, incomplete, or inaccurate records."
  },
  {
    "content": "What should a data architect do to provide additional guidance for users when they enter information in a standard field?",
    "options": [
      "A. Provide custom help text under field properties.",
      "B. Create a custom page with help text for user guidance.",
      "C. Add custom help text in default value for the field.",
      "D. Add a label field with help text adjacent to the custom field."
    ],
    "answer": "A",
    "title": "Question 45",
    "explanation": "The correct answer is A. To provide additional guidance for users when they enter information in a standard field, a data architect should provide custom help text under field properties. This will display a help icon next to the field label that users can hover over to see the help text. Option B is incorrect because creating a custom page with help text for user guidance will require additional development effort and may not be easily accessible by users. Option C is incorrect because adding custom help text in default value for the field will overwrite the actual default value of the field and may confuse users. Option D is incorrect because adding a label field with help text adjacent to the custom field will clutter the page layout and may not be visible to users."
  },
  {
    "content": "A customer wants to maintain geographic location information including latitude and longitude in a custom object. What would a data architect recommend to satisfy this requirement?",
    "options": [
      "A. Create formula fields with geolocation function for this requirement.",
      "B. Create custom fields to maintain latitude and longitude information",
      "C. Create a geolocation custom field to maintain this requirement",
      "D. Recommend app exchange packages to support this requirement."
    ],
    "answer": "C",
    "title": "Question 46",
    "explanation": "The correct answer is C, create a geolocation custom field to maintain this requirement. A geolocation custom field is a compound field that can store both latitude and longitude information in a single field. It also supports geolocation functions and distance calculations. Creating formula fields or custom fields for latitude and longitude separately would be inefficient and redundant. Recommending app exchange packages would not be a direct solution to the requirement."
  },
  {
    "content": "Universal Containers would like to have a Service-Level Agreement (SLA) of 1 day for any data loss due to unintentional or malicious updates of records in Salesforce. What approach should be suggested to address this requirement?",
    "options": [
      "A. Build a daily extract job and extract data to on-premise systems for long-term backup and archival purposes.",
      "B. Schedule a Weekly Extract Service for key objects and extract data in XL sheets to on-premise systems.",
      "C. Store all data in shadow custom objects on any updates and deletes, and extract them as needed .",
      "D. Evaluate a third-party AppExchange app, such as OwnBackup or Spanning, etc., for backup and archival purposes."
    ],
    "answer": "D",
    "title": "Question 47",
    "explanation": "Evaluating a third-party AppExchange app, such as OwnBackup or Spanning, etc., for backup and archival purposes is the best approach to address the requirement of having a one-day SLA for any data loss. These apps can provide automated backups, granular restores, and compliance features that can help UC recover from any data loss or corruption scenarios."
  },
  {
    "content": "US has released a new disaster recovery (DR)policy that states that cloud solutions need a business continuity plan in place separate from the cloud providers built in data recovery solution.\nWhich solution should a data architect use to comply with the DR policy?",
    "options": [
      "A. Leverage a 3rd party tool that extract salesforce data/metadata and stores the information in an external protected system.",
      "B. Leverage salesforce weekly exports, and store data in Flat files on a protected system.",
      "C. Utilize an ETL tool to migrate data to an on-premise archive solution.",
      "D. Write a custom batch job to extract data changes nightly, and store in an external protected system."
    ],
    "answer": "A",
    "title": "Question 48",
    "explanation": "The best solution to comply with the DR policy is to leverage a 3rd party tool that extract Salesforce data/metadata and stores the information in an external protected system. This solution can help create a backup of Salesforce data and metadata in case of a disaster or data loss event. It can also help restore data from the backup system to Salesforce if needed. There are various 3rd party tools available in the AppExchange or online that offer data backup and recovery services for Salesforce67. Leverage Salesforce weekly exports and store data in flat files on a protected system is not a good solution because it does not include metadata backup and it does not allow granular or automated data recovery. Utilize an ETL tool to migrate data to an on-premise archive solution is not a good solution because it does not include metadata backup and it may require complex data transformations and synchronizations. Write a custom batch job to extract data changes nightly and store in an external protected system is not a good solution because it does not include metadata backup and it may have performance or reliability issues."
  },
  {
    "content": "An Architect needs to document the data architecture for a multi-system, enterprise Salesforce implementation.\nWhich two key artifacts should the Architect use? (Choose two.)",
    "options": ["A. User stories", "B. Data model", "C. Integration specification", "D. Non-functional requirements"],
    "answer": "B,C",
    "title": "Question 49",
    "explanation": "Option B is correct because data model is a key artifact that an architect should use to document the data architecture for a multi-system, enterprise Salesforce implementation1. Data model describes the structure and relationship of data entities within an organization2. Option C is correct because integration specification is another key artifact that an architect should use to document the data architecture for a multi-system, enterprise Salesforce implementation1. Integration specification defines the scope, requirements, design, testing, and deployment of integration solutions between Salesforce and other systems3. Option A is not correct because user stories are not key artifacts for documenting the data architecture, but agile development tools that capture the features and functionalities that users want from a system4. Option D is not correct because non-functional requirements are not key artifacts for documenting the data architecture, but quality attributes that specify how well a system performs its functions."
  },
  {
    "content": "Universal Containers (UC) has a data model as shown in the image. The Project object has a private sharing model, and it has Roll -Up summary fields to calculate the number of resources assigned to the project, total hours for the project, and the number of work items associated to the project. What should the architect consider, knowing there will be a large amount of time entry records to be loaded regularly from an external system into Salesforce.com?",
    "options": [
      "A. Load all data using external IDs to link to parent records.",
      "B. Use workflow to calculate summary values instead of Roll -Up.",
      "C. Use triggers to calculate summary values instead of Roll -Up.",
      "D. Load all data after deferring sharing calculations."
    ],
    "answer": "D",
    "title": "Question 50",
    "explanation": "Loading all data after deferring sharing calculations can improve the performance and avoid locking issues when loading a large amount of time entry records into Salesforce.com. This is because deferring sharing calculations can temporarily suspend the calculation of sharing rules until all the data is loaded, and then recalculate them in one operation"
  },
  {
    "content": "Universal Containers (UC) owns a complex Salesforce org with many Apex classes, triggers, and automated processes that will modify records if available. UC has identified that, in its current development state, UC runs change of encountering race condition on the same record.\nWhat should a data architect recommend to guarantee that records are not being updated at the same time?",
    "options": [
      "A. Embed the keywords FOR UPDATE after SOQL statements.",
      "B. Disable classes or triggers that have the potential to obtain the same record.",
      "C. Migrate programmatic logic to processes and flows.",
      "D. Refactor or optimize classes and trigger for maximum CPU performance."
    ],
    "answer": "A",
    "title": "Question 51",
    "explanation": "According to the How to avoid row lock or race condition in Apex blog post, one of the ways to prevent race condition in Apex is to use the FOR UPDATE keyword in SOQL statements. The blog post states that \"We need to lock the records on which we are working such that other batches or threads will not be having any effect on them. How can we lock a record, then? We need to make use of FOR UPDATE keyword in the SOQL query.\" Therefore, a data architect should recommend this solution to guarantee that records are not being updated at the same time by different processes."
  },
  {
    "content": "NTO processes orders from its website via an order management system (OMS). The OMS stores over 2 million historical records and is currently not integrated with SF. The Sales team at NTO using Sales cloud and would like visibility into related customer orders yet they do not want to persist millions of records directly in Salesforce. NTO has asked the data architect to evaluate SF connect and the concept of data verification. Which 3 considerations are needed prior to a SF Connect implementation?\nChoose 3 answers:",
    "options": [
      "A. Create a 2nd system Admin user for authentication to the external source.",
      "B. Develop an object relationship strategy.",
      "C. Identify the external tables to sync into external objects",
      "D. Assess whether the external data source is reachable via an ODATA endpoint.",
      "E. Configure a middleware tool to poll external table data"
    ],
    "answer": "B,C,D",
    "title": "Question 52",
    "explanation": "The three considerations needed prior to a SF Connect implementation are to develop an object relationship strategy, identify the external tables to sync into external objects, and assess whether the external data source is reachable via an ODATA endpoint. SF Connect is a feature that allows integrating external data sources with Salesforce using external objects. External objects are similar to custom objects, but they store metadata only and not data. They enable on-demand access to external data via standard Salesforce APIs and user interfaces. To implement SF Connect, a data architect needs to consider how the external objects will relate to other objects in Salesforce, which external tables will be exposed as external objects, and whether the external data source supports ODATA protocol for data access."
  },
  {
    "content": "Company S was recently acquired by Company T. As part of the acquisition, all of the data for the Company S's Salesforce instance (source) must be migrated into the Company T's Salesforce instance (target). Company S has 6 million Case records.\nAn Architect has been tasked with optimizing the data load time.\nWhat should the Architect consider to achieve this goal?",
    "options": [
      "A. Pre-process the data, then use Data Loader with SOAP API to upsert with zip compression enabled.",
      "B. Directly leverage Salesforce-to-Salesforce functionality to load Case data.",
      "C. Load the data in multiple sets using Bulk API parallel processes.",
      "D. Utilize the Salesforce Org Migration Tool from the Setup Data Management menu."
    ],
    "answer": "A",
    "title": "Question 53",
    "explanation": "Pre-processing the data means transforming and cleansing the data before loading it into Salesforce. This can reduce the errors and conflicts that might occur during the data load. Using Data Loader with SOAP API to upsert with zip compression enabled can also improve the performance and efficiency of the data load by reducing the network bandwidth and avoiding duplication"
  },
  {
    "content": "NTO has implemented salesforce for its sales users. The opportunity management in salesforce is implemented as follows:\n1. Sales users enter their opportunities in salesforce for forecasting and reporting purposes.\n2. NTO has a product pricing system (PPS) that is used to update opportunity amount field on opportunities on a daily basis.\n3. PPS is the trusted source within the NTO for opportunity amount.\n4. NTO uses opportunity forecast for its sales planning and management.\nSales users have noticed that their updates to the opportunity amount field are overwritten when PPS updates their opportunities.\nHow should a data architect address this overriding issue?",
    "options": [
      "A. Create a custom field for opportunity amount that sales users update separating the fields that PPS updates.",
      "B. Create a custom field for opportunity amount that PPS updates separating the field that sales user updates.",
      "C. Change opportunity amount field access to read only for sales users using field level security.",
      "D. Change PPS integration to update only opportunity amount fields when values is NULL."
    ],
    "answer": "C",
    "title": "Question 54",
    "explanation": "Changing the opportunity amount field access to read only for sales users using field level security is the best way to address the overriding issue. This way, the sales users can still view the opportunity amount field but cannot edit it, and PPS can update it as the trusted source"
  },
  {
    "content": "Get Cloudy Consulting uses an invoicing system that has specific requirements. One requirement is that attachments associated with the Invoice_c custom object be classified by Types (i.e., \"\"Purchase Order\"\", \"\"Receipt\"\", etc.) so that reporting can be performed on invoices showing the number of attachments grouped by Type.\nWhat should an Architect do to categorize the attachments to fulfill these requirements?",
    "options": [
      "A. Add additional options to the standard ContentType picklist field for the Attachment object.",
      "B. Add a ContentType picklist field to the Attachment layout and create additional picklist options.",
      "C. Create a custom picklist field for the Type on the standard Attachment object with the values.",
      "D. Create a custom object related to the Invoice object with a picklist field for the Type."
    ],
    "answer": "D",
    "title": "Question 55",
    "explanation": "Creating a custom object related to the Invoice object with a picklist field for the Type allows the architect to categorize the attachments and report on them by Type. The standard Attachment object does not have a ContentType picklist field, and adding a custom picklist field to it would not be best practice."
  },
  {
    "content": "A customer is facing locking issued when importing large data volumes of order records that are children in a master-detail relationship with the Account object. What is the recommended way to avoid locking issues during import?",
    "options": [
      "A. Import Account records first followed by order records after sorting order by OrderID.",
      "B. Import Account records first followed by order records after sorting orders by AccountID.",
      "C. Change the relationship to Lookup and update the relationship to master-detail after import.",
      "D. Import Order records and Account records separately and populate AccountID in orders using batch Apex."
    ],
    "answer": "B",
    "title": "Question 56",
    "explanation": "Importing Account records first followed by order records after sorting orders by AccountID is the recommended way to avoid locking issues during import. This can reduce the number of lock contention errors by minimizing the number of parent records that are concurrently processed by multiple batches. Sorting orders by AccountID can also group the child records by their parent records and avoid updating the same parent record in different batches3. Importing Account records first followed by order records after sorting order by OrderID will not help with avoiding locking issues because it does not group the child records by their parent records. Changing the relationship to Lookup and updating the relationship to master-detail after import will not work because changing a relationship from Lookup to master-detail requires that all child records have a parent record, which may not be the case after import. Importing Order records and Account records separately and populating AccountID in orders using batch Apex will not help with avoiding locking issues because it still requires updating the parent records in batches."
  },
  {
    "content": "UC has multiple SF orgs that are distributed across regional branches. Each branch stores local customer data inside its org's Account and Contact objects. This creates a scenario where UC is unable to view customers across all orgs.\nUC has an initiative to create a 360-degree view of the customer, as UC would like to see Account and Contact data from all orgs in one place.\nWhat should a data architect suggest to achieve this 360-degree view of the customer?",
    "options": [
      "A. Consolidate the data from each org into a centralized datastore",
      "B. Use Salesforce Connect's cross-org adapter.",
      "C. Build a bidirectional integration between all orgs.",
      "D. Use an ETL tool to migrate gap Accounts and Contacts into each org."
    ],
    "answer": "A",
    "title": "Question 57",
    "explanation": "Consolidating the data from each org into a centralized datastore is the best suggestion to achieve a 360-degree view of the customer. This way, UC can have a single source of truth for all customer data and avoid data silos and inconsistencies. The other options are not feasible because they either require complex integration, additional cost, or data duplication."
  },
  {
    "content": "Universal Containers has a large volume of Contact data going into Salesforce.com. There are 100,000 existing contact records. 200,000 new contacts will be loaded. The Contact object has an external ID field that is unique and must be populated for all existing records. What should the architect recommend to reduce data load processing time?",
    "options": [
      "A. Load Contact records together using the Streaming API via the Upsert operation.",
      "B. Delete all existing records, and then load all records together via the Insert operation.",
      "C. Load all records via the Upsert operation to determine new records vs. existing records.",
      "D. Load new records via the Insert operation and existing records via the Update operation."
    ],
    "answer": "D",
    "title": "Question 58",
    "explanation": "Loading new records via the Insert operation and existing records via the Update operation will allow using the external ID field as a unique identifier and avoid any duplication or overwriting of records. This is faster and safer than deleting all existing records or using the Upsert operation, which might cause conflicts or errors ."
  },
  {
    "content": "An architect is planning on having different batches to load one million Opportunities into Salesforce using the Bulk API in parallel mode. What should be considered when loading the Opportunity records?",
    "options": [
      "A. Create indexes on Opportunity object text fields.",
      "B. Group batches by the AccountId field.",
      "C. Sort batches by Name field values.",
      "D. Order batches by Auto -number field."
    ],
    "answer": "D",
    "title": "Question 59",
    "explanation": "Ordering batches by Auto-number field will ensure that the records are processed in a sequential order and avoid any locking issues that might occur when loading related records in parallel mode. Creating indexes, grouping batches by AccountId, or sorting batches by Name field values are not necessary or beneficial for loading Opportunity records using the Bulk API."
  },
  {
    "content": "UC is having issues using Informatica Cloud Louder to export +10MOrder records. Each Order record has 10 Order Line Items. What two steps can you take to help correct this? Choose two answers.",
    "options": ["A. Export in multiple batches", "B. Export Bulk API in parallel mode", "C. Use PK Chunking", "D. Limit Batch to 10K records"],
    "answer": "A,C",
    "title": "Question 60",
    "explanation": "Exporting in multiple batches and using PK Chunking are two steps that can help correct the issues with exporting large volumes of Order records using Informatica Cloud Loader. Exporting in multiple batches can reduce the load on the system and avoid timeouts or errors. Using PK Chunking can split a large data set into smaller chunks based on the record IDs and enable parallel processing of each chunk."
  },
  {
    "content": "Universal Containers has millions of rows of data in Salesforce that are being used in reports to evaluate historical trends. Performance has become an issue, as well as data storage limits. Which two strategies should be recommended when talking with stakeholders?",
    "options": [
      "A. Use scheduled batch Apex to copy aggregate information into a custom object and delete the original records.",
      "B. Combine Analytics Snapshots with a purging plan by reporting on the snapshot data and deleting the original records.",
      "C. Use Data Loader to extract data, aggregate it, and write it back to a custom object, then delete the original records.",
      "D. Configure the Salesforce Archiving feature to archive older records and remove them from the data storage limits."
    ],
    "answer": "A,D",
    "title": "Question 61",
    "explanation": "Using scheduled batch Apex to copy aggregate information into a custom object and delete the original records can improve the performance and reduce the data storage limits by removing unnecessary data and keeping only the summary data that is needed for reporting. Configuring the Salesforce Archiving feature to archive older records and remove them from the data storage limits can also help with performance and storage issues by moving historical data to a separate system that is still accessible but does not affect the operational data"
  },
  {
    "content": "A consumer products company has decided to use Salesforce for its contact center. The contact center agents need access to the following information in Service Console when a customer contacts them:\n1. Customer browsing activity on its website stored on its on premise system\n2. Customer interactions with sales associates at its retail stores maintained in Salesforce\n3. Contact center interactions maintained in Salesforce\n4. Email campaign activity to customer from its marketing systems.\nWhat should a data architect do to fulfill these requirements with minimum development effort in Salesforce?",
    "options": [
      "A. Create web tabs in Service Console to show website and marketing activities.",
      "B. Build custom components in Service Console to bring information from the marketing and website information.",
      "C. Use Salesforce Connect to integrate the website and the marketing system into Service Console using external objects.",
      "D. Build customer view in Service Console with components that show website data and marketing data as mashup."
    ],
    "answer": "D",
    "title": "Question 62",
    "explanation": "Building a customer view in Service Console with components that show website data and marketing data as mashup is the best option to fulfill the requirement with minimum development effort in Salesforce. A mashup is a technique that combines data from different sources into a single user interface. You can use Visualforce pages or Lightning components to create mashups that display data from external systems such as your website and marketing system. This way, you can provide your contact center agents with a comprehensive view of the customer information they need."
  },
  {
    "content": "Northern trail Outfitters (NTO) uses Sales Cloud and service Cloud to manage sales and support processes. Some of NTOs team are complaining they see new fields on their page unsure of which values need be input. NTO is concerned about lack of governance in making changes to Salesforce.\nWhich governance measure should a data architect recommend to solve this issue?",
    "options": [
      "A. Add description fields to explain why the field is used, and mark the field as required.",
      "B. Create and manage a data dictionary and ups a governance process for changes made to common objects.",
      "C. Create reports to identify which users are leaving blank, and use external data sources o agreement the missing data.",
      "D. Create validation rules with error messages to explain why the fields is used"
    ],
    "answer": "B",
    "title": "Question 63",
    "explanation": "To solve the issue of lack of governance in making changes to Salesforce, a data architect should recommend creating and managing a data dictionary and setting up a governance process for changes made to common objects. A data dictionary is a document that defines the metadata, structure, and relationship of each object and field in Salesforce. A governance process is a set of rules and procedures that govern how changes are proposed, reviewed, approved, and deployed in Salesforce. These measures will help NTO to maintain consistency, quality, and clarity of their data model and avoid confusion and errors among users. Option A is incorrect because adding description fields to explain why the field is used, and marking the field as required will not prevent unauthorized or unnecessary changes to Salesforce. Option C is incorrect because creating reports to identify which users are leaving blank, and using external data sources to augment the missing data will not address the root cause of the issue, which is the lack of governance in making changes to Salesforce. Option D is incorrect because creating validation rules with error messages to explain why the fields are used will not stop users from seeing new fields on their page that they are unsure of."
  },
  {
    "content": "Universal Containers (UC) is concerned about the accuracy of their Customer information in Salesforce. They have recently created an enterprise-wide trusted source MDM for Customer data which they have certified to be accurate. UC has over 20 million unique customer records in the trusted source and Salesforce. What should an Architect recommend to ensure the data in Salesforce is identical to the MDM?",
    "options": [
      "A. Extract the Salesforce data into Excel and manually compare this against the trusted source.",
      "B. Load the Trusted Source data into Salesforce and run an Apex Batch job to find difference.",
      "C. Use an AppExchange package for Data Quality to match Salesforce data against the Trusted source.",
      "D. Leave the data in Salesforce alone and assume that it will auto-correct itself over time."
    ],
    "answer": "C",
    "title": "Question 64",
    "explanation": "Using an AppExchange package for Data Quality is a good way to match Salesforce data against a trusted source, such as an MDM system. You can use tools like Cloudingo, DupeCatcher, or DemandTools to identify and merge duplicate records, standardize data formats, and enrich data with external sources"
  },
  {
    "content": "UC is rolling out Sales App globally to bring sales teams together on one platform. UC expects millions of opportunities and accounts to be creates and is concerned about the performance of the application.\nWhich 3 recommendations should the data architect make to avoid the data skew? Choose 3 answers.",
    "options": [
      "A. Use picklist fields rather than lookup to custom object.",
      "B. Limit assigning one user 10000 records ownership.",
      "C. Assign 10000 opportunities to one account.",
      "D. Limit associating 10000 opportunities to one account.",
      "E. Limit associating 10000 records looking up to same records."
    ],
    "answer": "B,D,E",
    "title": "Question 65",
    "explanation": "Data skew occurs when a large number of child records are associated with a single parent record, or when a single user owns a large number of records. This can cause performance issues and lock contention. To avoid data skew, the data architect should limit assigning one user 10,000 records ownership, limit associating 10,000 opportunities to one account, and limit associating 10,000 records looking up to the same record"
  },
  {
    "content": "A Salesforce customer has plenty of data storage. Sales Reps are complaining that searches are bringing back old records that aren't relevant any longer. Sales Managers need the data for their historical reporting. What strategy should a data architect use to ensure a better user experience for the Sales Reps?",
    "options": [
      "A. Create a Permission Set to hide old data from Sales Reps.",
      "B. Use Batch Apex to archive old data on a rolling nightly basis.",
      "C. Archive and purge old data from Salesforce on a monthly basis.",
      "D. Set data access to Private to hide old data from Sales Reps."
    ],
    "answer": "C",
    "title": "Question 66",
    "explanation": "Archiving and purging old data from Salesforce on a monthly basis is a good strategy to improve the user experience for the Sales Reps, as it will reduce the clutter and improve the search performance. Creating a permission set or setting data access to private are not effective ways to hide old data from Sales Reps, as they will still consume data storage and affect search results. Using Batch Apex to archive old data on a rolling nightly basis is also not a good option, as it will consume API requests and processing time, and may not comply with the data retention policy."
  },
  {
    "content": "Northern Trail outfitters in migrating to salesforce from a legacy CRM system that identifies the agent relationships in a look-up table.\nWhat should the data architect do in order to migrate the data to Salesfoce?",
    "options": [
      "A. Create custom objects to store agent relationships.",
      "B. Migrate to Salesforce without a record owner.",
      "C. Assign record owner based on relationship.",
      "D. Migrate the data and assign to a non-person system user."
    ],
    "answer": "A",
    "title": "Question 67",
    "explanation": "The correct answer is A. To migrate the data to Salesforce, the data architect should create custom objects to store agent relationships. This will allow the data architect to replicate the look-up table structure from the legacy CRM system and maintain the relationship data in Salesforce. Option B is incorrect because migrating to Salesforce without a record owner will cause errors and prevent the data from being imported. Option C is incorrect because assigning record owner based on relationship will not preserve the agent relationships from the legacy CRM system. Option D is incorrect because migrating the data and assigning to a non-person system user will not allow the users to access and modify the data."
  },
  {
    "content": "Ursa Major Solar has defined a new Data Quality Plan for their Salesforce data.\nWhich two approaches should an Architect recommend to enforce the plan throughout the organization? (Choose two.)",
    "options": [
      "A. Ensure all data is stored in an external system and set up an integration to Salesforce for view-only access.",
      "B. Schedule reports that will automatically catch duplicates and merge or delete the records every week.",
      "C. Enforce critical business processes by using Workflow, Validation Rules, and Apex code.",
      "D. Schedule a weekly dashboard displaying records that are missing information to be sent to managers for review."
    ],
    "answer": "C,D",
    "title": "Question 68",
    "explanation": "Enforcing critical business processes by using Workflow, Validation Rules, and Apex code can help ensure data quality and consistency by applying rules and logic to the data entry and update3. Scheduling a weekly dashboard displaying records that are missing information to be sent to managers for review can help identify and fix data quality issues by providing visibility and accountability4."
  },
  {
    "content": "A large insurance provider is looking to implement Salesforce. The following exist.\n1. Multiple channel for lead acquisition\n2. Duplication leads across channels\n3. Poor customer experience and higher costs\nOn analysis, it found that there are duplicate leads that are resulting to mitigate the issues?",
    "options": [
      "A. Build process is manually search and merge duplicates.",
      "B. Standard lead information across all channels.",
      "C. Build a custom solution to identify and merge duplicate leads.",
      "D. Implement third-party solution to clean and event lead data.",
      "E. Implement de-duplication strategy to prevent duplicate leads"
    ],
    "answer": "B,D,E",
    "title": "Question 69",
    "explanation": "According to the Salesforce documentation2, duplicate leads are leads that have the same or similar information as other leads in Salesforce. Duplicate leads can cause poor customer experience, higher costs, and inaccurate reporting. To mitigate the issues caused by duplicate leads, some of the recommended practices are:\n Standardize lead information across all channels (option B). This means using consistent formats, values, and fields for capturing lead data from different sources, such as web forms, email campaigns, or third-party vendors. This can help reduce data quality issues and make it easier to identify and prevent duplicate leads.\n Implement a third-party solution to clean and enrich lead data (option D). This means using an external service or tool that can validate, correct, update, and enhance lead data before or after importing it into Salesforce. This can help improve data quality and accuracy, and reduce duplicate leads.\n Implement a de-duplication strategy to prevent duplicate leads (option E). This means using Salesforce features or custom solutions that can detect and block duplicate leads from being created or imported into Salesforce. For example, using Data.com Duplicate Management3, which allows defining matching rules and duplicate rules for leads and other objects.\n Building a process to manually search and merge duplicates (option A) is not a good practice, as it can be time-consuming, error-prone, and inefficient. Building a custom solution to identify and merge duplicate leads (option C) is also not a good practice, as it can be complex, costly, and difficult to maintain. It is better to use existing Salesforce features or third-party solutions that can handle duplicate leads more effectively."
  },
  {
    "content": "Universal Containers wants to automatically archive all inactive Account data that is older than 3 years. The information does not need to remain accessible within the application. Which two methods should be recommended to meet this requirement? Choose 2 answers",
    "options": [
      "A. Use the Force.com Workbench to export the data.",
      "B. Schedule a weekly export file from the Salesforce UI.",
      "C. Schedule jobs to export and delete using an ETL tool.",
      "D. Schedule jobs to export and delete using the Data Loader."
    ],
    "answer": "C,D",
    "title": "Question 70",
    "explanation": "Both C and D are valid methods to automatically archive and delete inactive Account data that is older than 3 years1. You can use an ETL tool or the Data Loader to schedule jobs to export and delete data based on certain criteria3. Option A is not recommended because the Force.com Workbench is a web-based tool that does not support scheduling or automation. Option B is not suitable because the weekly export file from the Salesforce UI does not delete data from Salesforce."
  },
  {
    "content": "A company has 12 million records, and a nightly integration queries these records.\nWhich two areas should a Data Architect investigate during troubleshooting if queries are timing out? (Choose two.)",
    "options": [
      "A. Make sure the query doesn't contain NULL in any filter criteria.",
      "B. Create a formula field instead of having multiple filter criteria.",
      "C. Create custom indexes on the fields used in the filter criteria.",
      "D. Modify the integration users' profile to have View All Data."
    ],
    "answer": "A,C",
    "title": "Question 71",
    "explanation": "Making sure the query does not contain NULL in any filter criteria can avoid full table scans and leverage indexes more efficiently. Queries with NULL filters are not selective and can cause performance issues. Creating custom indexes on the fields used in the filter criteria can also enhance the query performance by reducing the number of records to scan."
  },
  {
    "content": "Which two aspects of data does an Enterprise data governance program aim to improve?",
    "options": ["A. Data integrity", "B. Data distribution", "C. Data usability", "D. Data modeling"],
    "answer": "A,C",
    "title": "Question 72",
    "explanation": "Data integrity and data usability are two aspects of data that an Enterprise data governance program aims to improve. Data integrity refers to the accuracy, consistency, and validity of the data across the enterprise2. Data usability refers to the ease of access, analysis, and interpretation of the data by the end users"
  },
  {
    "content": "Universal Containers (UC) has 50 million customers and stores customer order history on an ERP system. UC also uses Salesforce to manage opportunities and customer support.\nIn order to provide seamless customer support, UC would like to see the customer's order history when viewing the customer record during a sales or support call.\nWhat should a data architect do in order to provide this functionality, while preserving the user experience?",
    "options": [
      "A. Use an Apex callout to populate a text area field for displaying the order history.",
      "B. Use Salesforce Connect and an external object to display the order history in Salesforce",
      "C. Import the order history into a custom Salesforce object, update nightly",
      "D. Embed the ERP system in an iframe and display on a custom tab."
    ],
    "answer": "B",
    "title": "Question 73",
    "explanation": "According to Trailhead2, Salesforce Connect is a feature that allows administrators to integrate external data sources with Salesforce without copying or synchronizing the data. Salesforce Connect can use external objects to display the external data in Salesforce as if it were stored in Salesforce objects, enabling seamless user experience and real-time access. Option B is the correct answer because it suggests using Salesforce Connect and an external object to display the order history in Salesforce. Option A is incorrect because using an Apex callout to populate a text area field for displaying the order history can be inefficient and unreliable, as it requires making an HTTP request for each customer record and storing the order history as unstructured text. Option C is incorrect because importing the order history into a custom Salesforce object, update nightly can consume a lot of storage space and cause data latency, as the order history may change more frequently than once a night. Option D is incorrect because embedding the ERP system in an iframe and displaying on a custom tab can degrade the user experience and security, as it requires switching between tabs and authenticating with another system."
  },
  {
    "content": "DreamHouse Realty has a Salesforce org that is used to manage Contacts.\nWhat are two things an Architect should consider using to maintain data quality in this situation? (Choose two.)",
    "options": [
      "A. Use the private sharing model.",
      "B. Use Salesforce duplicate management.",
      "C. Use validation rules on new record create and edit.",
      "D. Use workflow to delete duplicate records."
    ],
    "answer": "B,C",
    "title": "Question 74",
    "explanation": "Using Salesforce duplicate management and using validation rules on new record create and edit are two things that an architect should consider using to maintain data quality for managing Contacts. Salesforce duplicate management allows the architect to create matching rules and duplicate rules to identify, prevent, or allow duplicate records based on various criteria. Validation rules allow the architect to enforce data quality standards and business logic by displaying error messages when users try to save invalid data. The other options are not relevant or helpful for maintaining data quality"
  },
  {
    "content": "A casino is implementing salesforce and is planning to build a customer 360 view for a customer who visits its resorts. The casino currently maintained the following systems that records customer activity:\n1. Point of sales system: All purchases for a customer.\n2. Salesforce: All customer service activity and sales activity for a customer.\n3. Mobile app: All bookings, preferences and browser activity for a customer.\n4. Marketing: All email, SMS and social campaigns for a customer.\nCustomer service agents using salesforce would like to view the activities from all system to provide supports to customers. The information has to be current and real time.\nWhat strategy should the data architect implement to satisfy this requirement?",
    "options": [
      "A. Explore external data sources in salesforce to build 360 view of customer.",
      "B. Use a customer data mart to view the 360 view of customer.",
      "C. Migrate customer activities from all 4 systems into salesforce.",
      "D. Periodically upload summary information in salesforce to build 360 view."
    ],
    "answer": "A",
    "title": "Question 75",
    "explanation": "Exploring external data sources in Salesforce to build 360 view of customer is the best strategy to satisfy the requirement, as it allows real-time access to data from other systems without storing it in Salesforce3. Using a customer data mart may not provide real-time information or may require additional integration efforts. Migrating customer activities from all 4 systems into Salesforce may exceed the storage limits or cause data quality issues. Periodically uploading summary information in Salesforce may not provide current or detailed information."
  },
  {
    "content": "Cloud Kicks has the following requirements:\n* Their Shipment custom object must always relate to a Product, a Sender, and a Receiver (all separate custom objects).\n* If a Shipment is currently associated with a Product, Sender, or Receiver, deletion of those records should not be allowed.\n* Each custom object must have separate sharing models.\nWhat should an Architect do to fulfill these requirements?",
    "options": [
      "A. Associate the Shipment to each parent record by using a VLOOKUP formula field.",
      "B. Create a required Lookup relationship to each of the three parent records.",
      "C. Create a Master-Detail relationship to each of the three parent records.",
      "D. Create two Master-Detail and one Lookup relationship to the parent records."
    ],
    "answer": "B",
    "title": "Question 76",
    "explanation": "A required Lookup relationship ensures that the Shipment record must have a value for each of the three parent records, and also prevents the deletion of those parent records if they are referenced by a Shipment record. A Master-Detail relationship would not allow separate sharing models for each custom object, and a VLOOKUP formula field would not enforce the relationship or prevent deletion"
  },
  {
    "content": "UC has a legacy client server app that as a relational data base that needs to be migrated to salesforce.\nWhat are the 3 key actions that should be done when data modeling in salesforce?\nChoose 3 answers:",
    "options": [
      "A. Identify data elements to be persisted in salesforce.",
      "B. Map legacy data to salesforce objects.",
      "C. Map legacy data to salesforce custom objects.",
      "D. Work with legacy application owner to analysis legacy data model.",
      "E. Implement legacy data model within salesforce using custom fields."
    ],
    "answer": "A,B,E",
    "title": "Question 77",
    "explanation": "According to the Data Modeling unit on Trailhead, some of the key actions that should be done when data modeling in Salesforce are identifying data elements, mapping legacy data, and implementing legacy data model. The unit states that \"Before you start creating objects and fields in Salesforce, you need to identify the data elements that you want to store and work with. ... Next, you need to map your legacy data to Salesforce objects and fields. ... Finally, you need to implement your data model in Salesforce by creating custom objects and fields using declarative tools or Metadata API.\" Therefore, these are the correct actions for migrating a legacy client server app to Salesforce."
  },
  {
    "content": "A large multinational B2C Salesforce customer is looking to implement their distributor management application is Salesforce. the application has the following capabilities:\n1.Distributor create sales order in salesforce\n2.Sales order are based on product prices applicable to their region\n3. Sales order are closed once they are fulfilled\n4. It is decided to maintain the order in opportunity object\nHow should the data architect model this requirement?",
    "options": [
      "A. Create lookup to Custom Price object and share with distributors.",
      "B. Configure price Books for each region and share with distributors.",
      "C. Manually update Opportunities with Prices application to distributors.",
      "D. Add custom fields in Opportunity and use triggers to update prices."
    ],
    "answer": "B",
    "title": "Question 78",
    "explanation": "According to the Salesforce documentation, an opportunity is a standard object that represents a potential sale or deal with an account or contact. An opportunity can have products and prices associated with it using price books. A price book is a standard object that contains a list of products and their prices for different regions, currencies, segments, etc. A price book can be shared with different users or groups based on their visibility and access settings.\n To model the requirement of implementing a distributor management application in Salesforce, where distributors create sales orders based on product prices applicable to their region, and sales orders are closed once they are fulfilled, a data architect should:\n Configure price books for each region and share with distributors (option B). This means creating different price books for different regions with the appropriate products and prices, and sharing them with the distributors who belong to those regions. This way, distributors can create sales orders (opportunities) using the price books that are relevant to their region.\n Creating a lookup to Custom Price object and sharing with distributors (option A) is not a good solution, as it can introduce unnecessary complexity and redundancy to the data model. It is better to use standard objects and features that are designed for managing products and prices in Salesforce. Manually updating opportunities with prices applicable to distributors (option C) is also not a good solution, as it can be time-consuming, error-prone, and inefficient. It is better to use automation tools or features that can update prices based on predefined criteria or logic. Adding custom fields in opportunity and using triggers to update prices (option D) is also not a good solution, as it can be complex, costly, and difficult to maintain. It is better to use standard fields and features that can handle prices more effectively."
  },
  {
    "content": "Universal Containers has more than 10 million records in the Order_c object. The query has timed out when running a bulk query. What should be considered to resolve query timeout?",
    "options": ["A. Tooling API", "B. PK Chunking", "C. Metadata API", "D. Streaming API"],
    "answer": "B",
    "title": "Question 79",
    "explanation": "PK Chunking can resolve query timeout when running a bulk query on an object with more than 10 million records. PK Chunking is a feature of the Bulk API that splits a query into multiple batches based on the record IDs (primary keys) of the queried object. This can improve the query performance and avoid timeouts by reducing the number of records processed in each batch."
  },
  {
    "content": "Universal Container (UC) stores 10 million rows of inventory data in a cloud database, As part of creating a connected experience in Salesforce, UC would like to this inventory data to Sales Cloud without a import. UC has asked its data architect to determine if Salesforce Connect is needed.\nWhich three consideration should the data architect make when evaluating the need for Salesforce Connect?",
    "options": [
      "A. You want real-time access to the latest data, from other systems.",
      "B. You have a large amount of data and would like to copy subsets of it into Salesforce.",
      "C. You need to expose data via a virtual private connection.",
      "D. You have a large amount of data that you don't want to copy into your Salesforce org.",
      "E. You need to small amounts of external data at any one time."
    ],
    "answer": "A,D,E",
    "title": "Question 80",
    "explanation": "The correct answer is A, D, and E. The data architect should consider these three factors when evaluating the need for Salesforce Connect: You want real-time access to the latest data from other systems, you have a large amount of data that you don't want to copy into your Salesforce org, and you need to small amounts of external data at any one time. These factors indicate that Salesforce Connect is a suitable solution for creating a connected experience in Salesforce without importing inventory data from a cloud database. Salesforce Connect allows Salesforce to access external data via OData or custom adapters without storing it in Salesforce, which reduces storage costs and ensures data freshness. Salesforce Connect also supports pagination and caching to optimize performance when accessing small amounts of external data at any one time. Option B is incorrect because if you have a large amount of data and would like to copy subsets of it into Salesforce, you may not need Salesforce Connect but rather use other tools such as Data Loader or API integration. Option C is incorrect because if you need to expose data via a virtual private connection, you may not need Salesforce Connect but rather use other tools such as VPN or VPC peering."
  },
  {
    "content": "As part of a phased Salesforce rollout. there will be 3 deployments spread out over the year. The requirements have been carefully documented. Which two methods should an architect use to trace back configuration changes to the detailed requirements? Choose 2 answers",
    "options": [
      "A. Review the setup audit trail for configuration changes.",
      "B. Put the business purpose in the Description of each field.",
      "C. Maintain a data dictionary with the justification for each field.",
      "D. Use the Force.com IDE to save the metadata files in source control."
    ],
    "answer": "B,D",
    "title": "Question 81",
    "explanation": "Option B is correct because putting the business purpose in the Description of each field is a method that an architect can use to trace back configuration changes to the detailed requirements1. The Description of each field provides a brief explanation of what the field is used for and why it is needed2. Option D is correct because using the Force.com IDE to save the metadata files in source control is another method that an architect can use to trace back configuration changes to the detailed requirements1. The Force.com IDE is an integrated development environment that allows developers to work with Salesforce metadata files and Apex code3. Source control is a system that tracks and manages changes to code and configuration files4. Option A is not correct because reviewing the setup audit trail for configuration changes is not a method to trace back configuration changes to the detailed requirements, but a way to monitor and audit the changes made in the setup area. Option C is not correct because maintaining a data dictionary with the justification for each field is not a method to trace back configuration changes to the detailed requirements, but a document that provides information about the data entities and attributes in a system."
  },
  {
    "content": "Northern Trail Outfitters would like to report on the type of customers. A custom field for customer type was created in Account object. Users need to be limited to the following defined choices when entering information in this field:\n1. High Value\n2. Medium Value\n3. Low Value\nWhich strategy should a data architect recommend to configure customer type?",
    "options": [
      "A. Lookup to a custom object with defined choices.",
      "B. Single-select restricted picklist with defined choices.",
      "C. Provide help text to guide users with defined choices.",
      "D. Create a validation rule to limit entry to defined choices."
    ],
    "answer": "B",
    "title": "Question 82",
    "explanation": "single-select restricted picklist with defined choices can be a way to configure customer type. The article states that picklists are fields that allow users to select one or more predefined values from a list, and restricted picklists ensure that users can only select from the defined values. This can help to limit the choices for customer type and ensure data quality."
  },
  {
    "content": "Universal Containers (UC) has implemented Salesforce, UC is running out of storage and needs to have an archiving solution, UC would like to maintain two years of data in Saleforce and archive older data out of Salesforce.\nWhich solution should a data architect recommend as an archiving solution?",
    "options": [
      "A. Use a third-party backup solution to backup all data off platform.",
      "B. Build a batch join move all records off platform, and delete all records from Salesforce.",
      "C. Build a batch join to move two-year-old records off platform, and delete records from Salesforce.",
      "D. Build a batch job to move all restore off platform, and delete old records from Salesforce."
    ],
    "answer": "C",
    "title": "Question 83",
    "explanation": "The data architect should recommend building a batch job to move two-year-old records off platform, and delete records from Salesforce as an archiving solution. A batch job is a process that runs in the background and performs operations on large volumes of data in Salesforce. By building a batch job that moves two-year-old records off platform to an external storage system, such as Amazon S3 or Google Cloud Storage, and deletes them from Salesforce, the data architect can reduce the storage consumption and improve the performance of Salesforce org. Option A is incorrect because using a third-party backup solution to backup all data off platform will not free up any storage space in Salesforce, unless the data is also deleted from Salesforce after backup. Option B is incorrect because building a batch job to move all records off platform, and delete all records from Salesforce will result in losing all the current data in Salesforce, which may not be desirable or feasible. Option D is incorrect because building a batch job to move all restore off platform, and delete old records from Salesforce does not make sense, as restore implies restoring data back to Salesforce, not moving it off platform."
  },
  {
    "content": "UC migrating 100,000 Accounts from an enterprise resource planning (ERP) to salesforce and is concerned about ownership skew and performance.\nWhich 3 recommendations should a data architect provide to prevent ownership skew?\nChoose 3 answers:",
    "options": [
      "A. Assigned a default user as owner of accounts, and assign role in hierarchy.",
      "B. Keep users out of public groups that can be used as the source for sharing rules.",
      "C. Assign a default user as owner of account and do not assign any role to default user.",
      "D. Assign \"view all\" permission on profile to give access to account.",
      "E. Assign a default user as owner of accounts and assigned top most role in hierarchy."
    ],
    "answer": "B,C,E",
    "title": "Question 84",
    "explanation": "According to the Salesforce documentation1, ownership skew occurs when a large number of records (more than 10,000) are owned by a single user or queue. This can cause performance issues and lock contention when multiple users try to access or update those records. To prevent ownership skew, some of the recommended practices are:\n Assign a default user as the owner of the records and do not assign any role to the default user (option C). This way, the records will not be visible to other users in the role hierarchy and will not cause sharing recalculations.\n Keep users out of public groups that can be used as the source for sharing rules (option B). Sharing rules based on public groups can cause excessive sharing calculations and lock contention when many records are owned by a single user or queue.\n Assign a default user as the owner of the records and assign the top most role in the hierarchy to the default user (option E). This way, the records will be visible to all users in the role hierarchy, but will not cause sharing recalculations or lock contention.\n Assigning a default user as the owner of the records and assigning a role in the hierarchy (option A) is not a good practice, as it can cause sharing recalculations and lock contention when the role is updated or moved. Assigning \"view all\" permission on profile to give access to the records (option D) is also not a good practice, as it can bypass the security and sharing model and expose sensitive data to unauthorized users."
  },
  {
    "content": "A Customer is migrating 10 million order and 30 million order lines into Salesforce using Bulk API. The Engineer is experiencing time-out errors or long delays querying parents order IDs in Salesforce before importing related order line items. What is the recommended solution?",
    "options": [
      "A. Query only indexed ID field values on the imported order to import related order lines.",
      "B. Leverage an External ID from source system orders to import related order lines.",
      "C. Leverage Batch Apex to update order ID on related order lines after import.",
      "D. Leverage a sequence of numbers on the imported orders to import related order lines."
    ],
    "answer": "B",
    "title": "Question 85",
    "explanation": "Leverage an External ID from source system orders to import related order lines. This is the recommended solution because it allows you to use the upsert operation to match records based on the External ID field, which is indexed and unique. This avoids the need to query the parent order IDs in Salesforce before importing the order line items, which can cause time-out errors or long delays1."
  },
  {
    "content": "Universal Containers (UC) needs to run monthly and yearly reports on opportunities and orders for sales reporting. There are 5 million opportunities and 10 million orders. Sales users are complaining that the report will regularly timeout.\nWhat is the fastest and most effective way for a data architect to solve the time-out issue?",
    "options": [
      "A. Create custom fields on opportunity, and copy data from order into those custom fields and run all reports on Opportunity object.",
      "B. Extract opportunity and order data from Salesforce, and use a third-party reporting tool to run reports outside of Salesforce.",
      "C. Create a skinny table in Salesforce, and copy order and opportunity fields into the skinny table and create the required reports on It.",
      "D. Create an aggregate custom object that summarizes the monthly and yearly values into the required format for the required reports."
    ],
    "answer": "D",
    "title": "Question 86",
    "explanation": "Creating an aggregate custom object that summarizes the monthly and yearly values into the required format for the required reports (option D) is the fastest and most effective way for a data architect to solve the time-out issue, as it reduces the amount of data that needs to be queried and processed by the reports. Creating custom fields on opportunity and copying data from order into those custom fields (option A) is not a good solution, as it may create data redundancy and inconsistency, and it does not address the large volume of data. Extracting opportunity and order data from Salesforce and using a third-party reporting tool (option B) is also not a good solution, as it may introduce additional complexity and cost, and it does not leverage the native reporting features of Salesforce. Creating a skinny table in Salesforce and copying order and opportunity fields into it (option C) is also not a good solution, as it may not support all the reporting requirements, and it does not reduce the number of records."
  },
  {
    "content": "Universal Container is Implementing salesforce and needs to migrate data from two legacy systems. UC would like to clean and duplicate data before migrate to Salesforce.\nWhich solution should a data architect recommend a clean migration?",
    "options": [
      "A. Define external IDs for an object, migrate second database to first database, and load into Salesforce.",
      "B. Define duplicate rules in Salesforce, and load data into Salesforce from both databases.",
      "C. Set up staging data base, and define external IDs to merge, clean duplicate data, and load into Salesforce.",
      "D. Define external IDs for an object, Insert data from one database, and use upsert for a second database"
    ],
    "answer": "C",
    "title": "Question 87",
    "explanation": "To recommend a clean migration, the data architect should set up a staging database, and define external IDs to merge, clean duplicate data, and load into Salesforce. This will allow the data architect to consolidate and deduplicate the data from two legacy systems before importing it into Salesforce using external IDs as unique identifiers. Option A is incorrect because defining external IDs for an object, migrating second database to first database, and loading into Salesforce will not ensure that the data is clean and duplicate-free. Option B is incorrect because defining duplicate rules in Salesforce, and loading data into Salesforce from both databases will not prevent the duplicate data from being imported into Salesforce. Option D is incorrect because defining external IDs for an object, inserting data from one database, and using upsert for a second database will not handle the duplicate data from both databases."
  },
  {
    "content": "A manager at Cloud Kicks is importing Leads into Salesforce and needs to avoid creating duplicate records.\nWhich two approaches should the manager take to achieve this goal? (Choose two.)",
    "options": [
      "A. Acquire an AppExchange Lead de-duplication application.",
      "B. Implement Salesforce Matching and Duplicate Rules.",
      "C. Run the Salesforce Lead Mass de-duplication tool.",
      "D. Create a Workflow Rule to check for duplicate records."
    ],
    "answer": "A,B",
    "title": "Question 88",
    "explanation": "Acquiring an AppExchange Lead de-duplication application and implementing Salesforce Matching and Duplicate Rules are two approaches that the manager at Cloud Kicks should take to avoid creating duplicate records when importing Leads into Salesforce. An AppExchange Lead de-duplication application can provide additional features and functionality for finding and preventing duplicate Leads during import, such as fuzzy matching, custom rules, mass merge, etc. Salesforce Matching and Duplicate Rules can allow the manager to define how Salesforce identifies duplicate Leads based on various criteria and how users can handle them during import, such as blocking, allowing, or alerting them. The other options are not feasible or effective for avoiding duplicate records, as they would either not work during import, not provide de-duplication capabilities, or require additional customization."
  },
  {
    "content": "Northern Trail Outfitters is concerned because some of its data is sensitive and needs to be identified for access.\nWhat should be used to provide ways to filter and identify the sensitive data?",
    "options": [
      "A. Define data grouping metadata.",
      "B. Implement field-level security.",
      "C. Custom checkbox denoting sensitive data.",
      "D. Define data classification metadata."
    ],
    "answer": "D",
    "title": "Question 89",
    "explanation": "According to the official Salesforce guide1, data classification metadata is a feature that allows administrators to classify data fields based on their sensitivity level, such as confidential, restricted, or general. Data classification metadata can be used to filter and identify sensitive data fields and apply appropriate security measures, such as encryption, masking, or auditing. Option D is the correct answer because it suggests using data classification metadata to provide ways to filter and identify sensitive data. Option A is incorrect because data grouping metadata is not a feature in Salesforce. Option B is incorrect because field-level security is a feature that controls the visibility and editability of fields based on user profiles or permission sets, but it does not provide ways to filter and identify sensitive data. Option C is incorrect because creating a custom checkbox denoting sensitive data is not a scalable or reliable solution, as it requires manual maintenance and does not enforce any security measures."
  },
  {
    "content": "Universal container (UC) would like to build a Human resources application on Salesforce to manage employee details, payroll, and hiring efforts. To adequately and store the relevant data, the application will need to leverage 45 custom objects. In addition to this, UC expects roughly 20,00 API calls into Salesfoce from an n-premises application daily.\nWhich license type should a data architect recommend that best fits these requirements?",
    "options": ["A. Service Cloud", "B. Lightning platform Start", "C. Lightning Platform plus", "D. Lightning External Apps Starts"],
    "answer": "C",
    "title": "Question 90",
    "explanation": "Lightning Platform Plus is the license type that best fits UC's requirements, as it allows up to 50 custom objects and 40,000 API calls per user per 24-hour period4. Service Cloud does not provide enough custom objects or API calls. Lightning Platform Start only allows up to 10 custom objects and 5,000 API calls per user per 24-hour period. Lightning External Apps Start is for external users and does not provide enough API calls."
  },
  {
    "content": "Universal Containers (UC) is implementing a Salesforce project with large volumes of data and daily transactions. The solution includes both real-time web service integrations and Visualforce mash -ups with back -end systems. The Salesforce Full sandbox used by the project integrates with full-scale back -end testing systems. What two types of performance testing are appropriate for this project?\nChoose 2 answers",
    "options": [
      "A. Pre -go -live automated page -load testing against the Salesforce Full sandbox.",
      "B. Post go -live automated page -load testing against the Salesforce Production org.",
      "C. Pre -go -live unit testing in the Salesforce Full sandbox.",
      "D. Stress testing against the web services hosted by the integration middleware."
    ],
    "answer": "A,D",
    "title": "Question 91",
    "explanation": "Pre-go-live automated page-load testing against the Salesforce Full sandbox can help identify and resolve any performance bottlenecks or issues before deploying the solution to production. The Full sandbox is an ideal environment for performance testing as it replicates the production org in terms of data, metadata, and integrations. Stress testing against the web services hosted by the integration middleware can also help evaluate the scalability and reliability of the integration solution under high load conditions."
  },
  {
    "content": "DreamHouse Realty has a Salesforce deployment that manages Sales, Support, and Marketing efforts in a multi-system ERP environment. The company recently reached the limits of native reports and dashboards and needs options for providing more analytical insights.\nWhat are two approaches an Architect should recommend? (Choose two.)",
    "options": ["A. Weekly Snapshots", "B. Einstein Analytics", "C. Setup Audit Trails", "D. AppExchange Apps"],
    "answer": "B,D",
    "title": "Question 92",
    "explanation": "Einstein Analytics can provide more analytical insights than native reports and dashboards by allowing users to explore data from multiple sources, create interactive visualizations, and apply AI-powered features5. AppExchange Apps can also provide more analytical insights by offering pre-built solutions or integrations with external tools that can enhance the reporting and analytics capabilities of Salesforce6."
  },
  {
    "content": "Universal Container is using Salesforce for Opportunity management and enterprise resource planning (ERP) for order management. Sales reps do not have access to the ERP and have no visibility into order status.\nWhat solution a data architect recommends to give the sales team visibility into order status?",
    "options": [
      "A. Leverage Canvas to bring the order management UI in to the Salesforce tab.",
      "B. Build batch jobs to push order line items to salesforce.",
      "C. leverage Salesforce Connect top bring the order line item from the legacy system to Salesforce.",
      "D. Build real-time integration to pull order line items into Salesforce when viewing orders."
    ],
    "answer": "C",
    "title": "Question 93",
    "explanation": "The correct answer is C, leverage Salesforce Connect to bring the order line item from the legacy system to Salesforce. Salesforce Connect is a feature that allows you to integrate external data sources with Salesforce and access them in real time without copying or synchronizing the data. This way, the sales team can view the order status from the ERP system without having access to it. Leveraging Canvas, building batch jobs, or building real-time integration are also possible solutions, but they are more complex and costly than using Salesforce Connect."
  },
  {
    "content": "North Trail Outfitters (NTO) operates a majority of its business from a central Salesforce org, NTO also owns several secondary orgs that the service, finance, and marketing teams work out of, At the moment, there is no integration between central and secondary orgs, leading to data-visibility issues.\nMoving forward, NTO has identified that a hub-and-spoke model is the proper architect to manage its data, where the central org is the hub and the secondary orgs are the spokes.\nWhich tool should a data architect use to orchestrate data between the hub org and spoke orgs?",
    "options": [
      "A. A middleware solution that extracts and distributes data across both the hub and spokes.",
      "B. Develop custom APIs to poll the hub org for change data and push into the spoke orgs.",
      "C. Develop custom APIs to poll the spoke for change data and push into the org.",
      "D. A backup and archive solution that extracts and restores data across orgs."
    ],
    "answer": "A",
    "title": "Question 94",
    "explanation": "According to the Salesforce documentation, a hub-and-spoke model is an integration architecture pattern that allows connecting multiple Salesforce orgs using a central org (hub) and one or more secondary orgs (spokes). The hub org acts as the master data source and orchestrates the data flow between the spoke orgs. The spoke orgs act as the consumers or producers of the data and communicate with the hub org.\n To orchestrate data between the hub org and spoke orgs, a data architect should use:\n A middleware solution that extracts and distributes data across both the hub and spokes (option A). This means using an external service or tool that can connect to multiple Salesforce orgs using APIs or connectors, and perform data extraction, transformation, and distribution operations between the hub and spoke orgs. This can provide a scalable, flexible, and reliable way to orchestrate data across multiple orgs.\n Developing custom APIs to poll the hub org for change data and push into the spoke orgs (option B) is not a good solution, as it can be complex, costly, and difficult to maintain. It may also not be able to handle large volumes of data or complex transformations efficiently. Developing custom APIs to poll the spoke orgs for change data and push into the hub org (option C) is also not a good solution, as it can have the same drawbacks as option B. It may also not be able to handle conflicts or errors effectively. Using a backup and archive solution that extracts and restores data across orgs (option D) is also not a good solution, as it can incur additional costs and dependencies. It may also not be able to handle real-time or near-real-time data orchestration requirements."
  },
  {
    "content": "Universal Containers (UC) has lead assignment rules to assign leads to owners. Leads not routed by assignment rules are assigned to a dummy user. Sales rep are complaining of high load times and issues with accessing leads assigned to the dummy user.\nWhat should a data architect recommend to solve these performance issues?",
    "options": [
      "A. Assign dummy user last role in role hierarchy",
      "B. Create multiple dummy user and assign leads to them",
      "C. Assign dummy user to highest role in role hierarchy",
      "D. Periodically delete leads to reduce number of leads"
    ],
    "answer": "B",
    "title": "Question 95",
    "explanation": "According to the official Salesforce guide1, assigning leads to a single dummy user can cause performance issues and data skew, especially if the dummy user owns more than 10,000 records. Data skew occurs when a single user or a small number of users own a disproportionately large number of records, which can affect query performance and sharing calculations. Option B is the correct answer because it suggests creating multiple dummy users and assigning leads to them, which can distribute the load and reduce data skew. Option A is incorrect because assigning the dummy user to the last role in the role hierarchy does not affect the performance or data skew issues. Option C is incorrect because assigning the dummy user to the highest role in the role hierarchy can worsen the performance and data skew issues, as it will grant access to more users and records. Option D is incorrect because periodically deleting leads can cause data loss and does not address the root cause of the problem."
  },
  {
    "content": "Universal Containers (UC) is using Salesforce Sales & Service Cloud for B2C sales and customer service but they are experiencing a lot of duplicate customers in the system. Which are two recommended approaches for UC to avoid duplicate data and increase the level of data quality?",
    "options": ["A. Use Duplicate Management.", "B. Use an Enterprise Service Bus.", "C. Use Data.com Clean", "D. Use a data warehouse."],
    "answer": "A,C",
    "title": "Question 96",
    "explanation": "Using Duplicate Management and Data.com Clean are two recommended approaches for UC to avoid duplicate data and increase the level of data quality. Duplicate Management can prevent or alert users when they try to create or edit records that are duplicates of existing records. Data.com Clean can compare Salesforce records with Data.com records and provide suggestions for updates or removals of duplicate records."
  },
  {
    "content": "Universal Containers (UC) needs to move millions of records from an external enterprise resource planning (ERP) system into Salesforce.\nWhat should a data architect recommend to be done while using the Bulk API in serial mode instead of parallel mode?",
    "options": [
      "A. Placing 20 batches on the queue for upset jobs.",
      "B. Inserting 1 million orders distributed across a variety of accounts with potential lock exceptions.",
      "C. Leveraging a controlled feed load with 10 batches per job.",
      "D. Inserting 1 million orders distributed across a variety of accounts with lock exceptions eliminated and managed."
    ],
    "answer": "B",
    "title": "Question 97",
    "explanation": "According to this article, inserting 1 million orders distributed across a variety of accounts with potential lock exceptions is a scenario where using the Bulk API in serial mode can help to prevent the lock contention issue that can occur in parallel mode."
  },
  {
    "content": "Universal Containers (UC) is implementing Salesforce and will be using Salesforce to track customer complaints, provide white papers on products, and provide subscription-based support.\nWhich license type will UC users need to fulfill UC's requirements?",
    "options": ["A. Sales Cloud License", "B. Lightning Platform Starter License", "C. Service Cloud License", "D. Salesforce License"],
    "answer": "C",
    "title": "Question 98",
    "explanation": "Service Cloud License (option C) is the license type that UC users need to fulfill UC's requirements, as it allows them to track customer complaints, provide white papers on products, and provide subscription-based support. Sales Cloud License (option A) is mainly for managing sales processes and leads, Lightning Platform Starter License (option B) is for building custom apps and workflows, and Salesforce License (option D) is a generic term that does not specify a particular license type."
  },
  {
    "content": "Universal Containers (UC) has built a custom application on Salesforce to help track shipments around the world. A majority of the shipping records are stored on premise in an external data source. UC needs shipment details to be exposed to the custom application, and the data needs to be accessible in real time. The external data source is not OData enabled, and UC does not own a middleware tool.\nWhich Salesforce Connect procedure should a data architect use to ensure UC's requirements are met?",
    "options": [
      "A. Write an Apex class that makes a REST callout to the external API.",
      "B. Develop a process that calls an inviable web service method.",
      "C. Migrate the data to Heroku and register Postgres as a data source.",
      "D. Write a custom adapter with the Apex Connector Framework."
    ],
    "answer": "D",
    "title": "Question 99",
    "explanation": "According to this article, the Apex Connector Framework enables developers to create custom adapters for Salesforce Connect to access data from external systems that are not OData enabled. This can meet UC's requirements of exposing shipment details to the custom application and accessing the data in real time."
  },
  {
    "content": "UC is implementing sales cloud for patient management and would like to encrypt sensitive patient records being stored in files.\nWhich solution should a data architect recommend to solve this requirement?",
    "options": [
      "A. Implement shield platform encryption to encrypt files.",
      "B. Use classic encryption to encrypt files.",
      "C. Implement 3rd party App Exchange app to encrypt files.",
      "D. Store files outside of salesforce and access them to real time."
    ],
    "answer": "A",
    "title": "Question 100",
    "explanation": "Shield platform encryption is the recommended solution for encrypting sensitive patient records stored in files, as it provides encryption at rest for files and attachments, as well as standard and custom fields. Classic encryption only supports text fields with a maximum length of 175 characters, and does not encrypt files. Third-party App Exchange apps may not provide the same level of security and compliance as shield platform encryption. Storing files outside of salesforce may introduce additional complexity and latency"
  },
  {
    "content": "A shipping and logistics company has created a large number of reports within Sales Cloud since Salesforce was introduced. Some of these reports analyze large amounts of data regarding the whereabouts of the company's containers, and they are starting to time out when users are trying to run the reports. What is a recommended approach to avoid these time-out issues?",
    "options": [
      "A. Improve reporting performance by creating a custom Visualforce report that is using a cache of the records in the report.",
      "B. Improve reporting performance by replacing the existing reports in Sales Cloud with new reports based on Analytics Cloud.",
      "C. Improve reporting performance by creating an Apex trigger for the Report object that will pre-fetch data before the report is run.",
      "D. Improve reporting performance by creating a dashboard that is scheduled to run the reports only once per day."
    ],
    "answer": "B",
    "title": "Question 101",
    "explanation": "Improving reporting performance by replacing the existing reports in Sales Cloud with new reports based on Analytics Cloud can avoid the time-out issues by leveraging the power and scalability of Analytics Cloud. Analytics Cloud can handle large volumes of data and provide faster and more interactive reports than Sales Cloud ."
  },
  {
    "content": "UC is planning a massive SF implementation with large volumes of dat\na. As part of the org's implementation, several roles, territories, groups, and sharing rules have been configured. The data architect has been tasked with loading all of the required data, including user data, in a timely manner.\nWhat should a data architect do to minimize data load times due to system calculations?",
    "options": [
      "A. Enable defer sharing calculations, and suspend sharing rule calculations",
      "B. Load the data through data loader, and turn on parallel processing.",
      "C. Leverage the Bulk API and concurrent processing with multiple batches",
      "D. Enable granular locking to avoid \"UNABLE _TO_LOCK_ROW\" error."
    ],
    "answer": "A",
    "title": "Question 102",
    "explanation": "The correct answer is A, enable defer sharing calculations, and suspend sharing rule calculations. Defer sharing calculations and suspend sharing rule calculations are features that allow you to temporarily disable the automatic recalculation of sharing rules when you load large volumes of data. This can improve the performance and speed of your data load process by avoiding unnecessary system calculations. Loading the data through data loader, leveraging the bulk API, or enabling granular locking are also options that can help with data load times, but they do not directly address the system calculations issue."
  },
  {
    "content": "UC recently migrated 1 Billion customer related records from a legacy data store to Heroku Postgres. A subset of the data need to be synchronized with salesforce so that service agents are able to support customers directly within the service console. The remaining non- synchronized set of data will need to be accessed by salesforce at any point in time, but UC management is concerned about storage limitations.\nWhat should a data architect recommend to meet these requirements with minimal effort?",
    "options": [
      "A. Virtualize the remaining set of data with salesforce connect and external objects.",
      "B. Use Heroku connect to bi-directional, sync all data between systems.",
      "C. As needed, make call outs into Heroku postgres and persist the data in salesforce.",
      "D. Migrate the data to big objects and leverage async SOQL with custom objects."
    ],
    "answer": "A",
    "title": "Question 103",
    "explanation": "Virtualizing the remaining set of data with salesforce connect and external objects is the best way to meet the requirements with minimal effort, as it allows salesforce to access data stored in Heroku Postgres without storing it in salesforce. This reduces the storage limitations and avoids data duplication. Heroku connect can bi-directionally sync data between systems, but it requires more configuration and maintenance. Making callouts to Heroku Postgres and persisting the data in salesforce may not be feasible for 1 billion records. Migrating the data to big objects may incur additional costs and require custom code to use async SOQL"
  },
  {
    "content": "Universal Containers uses Apex jobs to create leads in Salesforce. The business team has requested that lead creation failures should be reported to them.\nWhich option does Apex provide to report errors from this Apex job?",
    "options": [
      "A. Use Custom Object to store leads, and allow unprocessed leads to be reported.",
      "B. Save Apex errors in a custom object, and allow access to this object for reporting.",
      "C. Use Apex services to email failures to business when error occurs.",
      "D. Use AppExchange package to clean lead information before Apex job processes them."
    ],
    "answer": "C",
    "title": "Question 104",
    "explanation": "saving Apex errors in a custom object can be a way to report errors from an Apex job. The article provides an example of how to create a custom object called AsyncApexError__c and use a trigger to insert error records into it. The custom object can then be used for reporting or notification purposes."
  },
  {
    "content": "UC has large amount of orders coming in from its online portal. Historically all order are assigned to a generic user.\nWhich 2 measures should data architect recommend to avoid any performance issues while working with large number of order records? Choose 2 answers:",
    "options": [
      "A. Clear the role field in the generic user record.",
      "B. Salesforce handles the assignment of orders automatically and there is no performance impact.",
      "C. Create a role at top of role hierarchy and assign the role to the generic user.",
      "D. Create a pool of generic users and distribute the assignment of memory to the pool of users."
    ],
    "answer": "A,C",
    "title": "Question 105",
    "explanation": "Clearing the role field in the generic user record and creating a role at the top of the role hierarchy and assigning it to the generic user are two measures that can help avoid performance issues while working with large number of order records. These measures can prevent data skew and lock contention that may occur when a single user owns or shares a large number of records"
  },
  {
    "content": "Get Cloud Consulting needs to integrate two different systems with customer records into the Salesforce Account object. So that no duplicate records are created in Salesforce, Master Data Management will be used.\nAn Architect needs to determine which system is the system of record on a field level.\nWhat should the Architect do to achieve this goal?",
    "options": [
      "A. Master Data Management systems determine system of record, and the Architect doesn't have to think about what data is controlled by what system.",
      "B. Key stakeholders should review any fields that share the same purpose between systems to see how they will be used in Salesforce.",
      "C. The database schema for each external system should be reviewed, and fields with different names should always be separate fields in Salesforce.",
      "D. Any field that is an input field in either external system will be overwritten by the last record integrated and can never have a system of record."
    ],
    "answer": "B",
    "title": "Question 106",
    "explanation": "Key stakeholders from both systems should collaborate with the Architect to determine which system is the system of record on a field level is what the Architect should do to achieve this goal of integrating two different systems with customer records into the Salesforce Account object using Master Data Management. The system of record is the authoritative source of truth for a given entity or field in a given context. Different systems may have different levels of accuracy, completeness, timeliness, or relevance for different fields. Therefore, it is important to involve key stakeholders from both systems who have knowledge and expertise about their data quality and business needs to decide which system should be the system of record for each field. The Architect should facilitate this collaboration and document the decisions and rationale for each field. The other options are not correct or feasible, as they would either delegate or abdicate the responsibility of determining the system of record, ignore or disregard the input from key stakeholders, or assume or impose a default system of record without considering the data quality and business needs."
  },
  {
    "content": "Universal Containers (UC) is in the process of migrating legacy inventory data from an enterprise resources planning (ERP) system into Sales Cloud with the following requirements:\nLegacy inventory data will be stored in a custom child object called Inventory_c.\nInventory data should be related to the standard Account object.\nThe Inventory object should Invent the same sharing rules as the Account object.\nAnytime an Account record is deleted in Salesforce, the related Inventory_c record(s) should be deleted as well.\nWhat type of relationship field should a data architect recommend in this scenario?",
    "options": [
      "A. Master-detail relationship filed on Account, related to Inventory_c",
      "B. Master-detail relationship filed on Inventory_c, related to Account",
      "C. Indirect lookup relationship field on Account, related to Inventory_c",
      "D. Lookup relationship fields on Inventory related to Account"
    ],
    "answer": "B",
    "title": "Question 107",
    "explanation": "According to the Salesforce documentation, a relationship field is a field that allows linking one object to another object in Salesforce. There are different types of relationship fields that have different characteristics and behaviors, such as master-detail, lookup, indirect lookup, external lookup, etc.\n To recommend a type of relationship field for this scenario, where legacy inventory data will be stored in a custom child object called Inventory__c, inventory data should be related to the standard Account object, the Inventory__c object should inherit the same sharing rules as the Account object, and anytime an Account record is deleted in Salesforce, the related Inventory__c record(s) should be deleted as well, a data architect should recommend:\n Master-detail relationship field on Inventory__c, related to Account (option B). This means creating a field on the Inventory__c object that references the Account object as its parent. A master-detail relationship field establishes a parent-child relationship between two objects, where the parent object controls certain behaviors of the child object. For example, a master-detail relationship field can:\n Inherit the sharing and security settings from the parent object to the child object. This means that the users who can access and edit the parent record can also access and edit the related child records.\n Cascade delete from the parent object to the child object. This means that when a parent record is deleted, all the related child records are also deleted.\n Roll up summary fields from the child object to the parent object. This means that the parent object can display aggregated information from the child records, such as count, sum, min, max, or average.\n Master-detail relationship field on Account, related to Inventory__c (option A) is not a good solution, as it reverses the direction of the relationship. This means creating a field on the Account object that references the Inventory__c object as its parent. This is not possible, as a standard object cannot be on the detail side of a master-detail relationship. Indirect lookup relationship field on Account, related to Inventory__c (option C) is also not a good solution, as it is a special type of relationship field that allows linking a custom object to a standard object on an external system using an indirect reference. This is not applicable for this scenario, as both objects are in Salesforce and do not need an external reference. Lookup relationship field on Inventory__c related to Account (option D) is also not a good solution, as it establishes a looser relationship between two objects than a master-detail relationship. A lookup relationship field does not inherit sharing and security settings from the parent object to the child object, does not cascade delete from the parent object to the child object, and does not roll up summary fields from the child object to the parent object."
  },
  {
    "content": "NTO (Northern Trail Outlets) has a complex Salesforce org which has been developed over past 5 years. Internal users are complaining abt multiple data issues, including incomplete and duplicate data in the org. NTO has decided to engage a data architect to analyze and define data quality standards.\nWhich 3 key factors should a data architect consider while defining data quality standards? Choose 3 answers:",
    "options": [
      "A. Define data duplication standards and rules",
      "B. Define key fields in staging database for data cleansing",
      "C. Measure data timeliness and consistency",
      "D. Finalize an extract transform load (ETL) tool for data migration",
      "E. Measure data completeness and accuracy"
    ],
    "answer": "A,C,E",
    "title": "Question 108",
    "explanation": "Defining data duplication standards and rules, measuring data timeliness and consistency, and measuring data completeness and accuracy are three key factors that a data architect should consider while defining data quality standards. Defining data duplication standards and rules can help prevent or reduce duplicate records in the org by specifying criteria and actions for identifying and merging duplicates. Measuring data timeliness and consistency can help ensure that the data is up-to-date, reliable, and synchronized across different sources. Measuring data completeness and accuracy can help ensure that the data is sufficient, relevant, and correct for the intended purposes."
  },
  {
    "content": "Universal Containers is planning out their archiving and purging plans going forward for their custom objects Topic__c and Comment__c. Several options are being considered, including analytics snapshots, offsite storage, scheduled purges, etc. Which three questions should be considered when designing an appropriate archiving strategy?",
    "options": [
      "A. How many fields are defined on the custom objects that need to be archived?",
      "B. Which profiles and users currently have access to these custom object records?",
      "C. If reporting is necessary, can the information be aggregated into fewer, summary records?",
      "D. Will the data being archived need to be reported on or accessed in any way in the future?",
      "E. Are there any regulatory restrictions that will influence the archiving and purging plans?"
    ],
    "answer": "C,D,E",
    "title": "Question 109",
    "explanation": "The three questions that should be considered when designing an appropriate archiving strategy are: If reporting is necessary, can the information be aggregated into fewer, summary records? Will the data being archived need to be reported on or accessed in any way in the future? Are there any regulatory restrictions that will influence the archiving and purging plans? These questions are important because they help determine the scope, frequency, and method of archiving and purging data from Salesforce. For example, if reporting is necessary, then summary records or analytics snapshots can be used to store aggregated data and reduce the number of records that need to be archived1. If the data being archived needs to be accessed in the future, then offsite storage or external objects can be used to retain the data and make it available on demand2. If there are any regulatory restrictions, such as GDPR or HIPAA, then the archiving and purging plans need to comply with them and ensure data security and privacy"
  },
  {
    "content": "Cloud Kicks is launching a Partner Community, which will allow users to register shipment requests that are then processed by Cloud Kicks employees. Shipment requests contain header information, and then a list of no more than 5 items being shipped.\nFirst, Cloud Kicks will introduce its community to 6,000 customers in North America, and then to 24,000 customers worldwide within the next two years. Cloud Kicks expects 12 shipment requests per week per customer, on average, and wants customers to be able to view up to three years of shipment requests and use Salesforce reports.\nWhat is the recommended solution for the Cloud Kicks Data Architect to address the requirements?",
    "options": [
      "A. Create an external custom object to track shipment requests and a child external object to track shipment items. External objects are stored off-platform in Heroku's Postgres database.",
      "B. Create an external custom object to track shipment requests with five lookup custom fields for each item being shipped. External objects are stored off-platform in Heroku's Postgres database.",
      "C. Create a custom object to track shipment requests and a child custom object to track shipment items. Implement an archiving process that moves data off-platform after three years.",
      "D. Create a custom object to track shipment requests with five lookup custom fields for each item being shipped Implement an archiving process that moves data off-platform after three years."
    ],
    "answer": "C",
    "title": "Question 110",
    "explanation": "The recommended solution for the Cloud Kicks Data Architect to address the requirements is to create a custom object to track shipment requests and a child custom object to track shipment items. Implement an archiving process that moves data off-platform after three years. This solution would allow Cloud Kicks to store and manage their shipment data on Salesforce, and use Salesforce reports to analyze it. However, since Cloud Kicks expects a large volume of data over time, they should implement an archiving process that moves data off-platform after three years to avoid hitting the Org data storage limit and maintain optimal performance3. External objects are not a good option for this scenario, because they are stored off-platform in an external system, such as Heroku's Postgres database, and they have limited functionality and performance compared to custom objects"
  },
  {
    "content": "A data architect has been tasked with optimizing a data stewardship engagement for a Salesforce instance Which three areas of Salesforce should the architect review before proposing any design recommendation? Choose 3 answers",
    "options": [
      "A. Review the metadata xml files for redundant fields to consolidate.",
      "B. Determine if any integration points create records in Salesforce.",
      "C. Run key reports to determine what fields should be required.",
      "D. Export the setup audit trail to review what fields are being used.",
      "E. Review the sharing model to determine impact on duplicate records."
    ],
    "answer": "B,C,E",
    "title": "Question 111",
    "explanation": "Determining if any integration points create records in Salesforce, running key reports to determine what fields should be required, and reviewing the sharing model to determine impact on duplicate records are three areas of Salesforce that the architect should review before proposing any design recommendation. These areas will help to identify the sources and quality of data, the business rules and validations for data entry, and the access and visibility of data across users and roles. Reviewing the metadata xml files for redundant fields to consolidate, and exporting the setup audit trail to review what fields are being used are not necessary steps for optimizing a data stewardship engagement, as they are more related to metadata management and audit tracking."
  },
  {
    "content": "Universal Containers has a large number of Opportunity fields (100) that they want to track field history on. Which two actions should an architect perform in order to meet this requirement? Choose 2 answers",
    "options": [
      "A. Create a custom object to store a copy of the record when changed.",
      "B. Create a custom object to store the previous and new field values.",
      "C. Use Analytic Snapshots to store a copy of the record when changed.",
      "D. Select the 100 fields in the Opportunity Set History Tracking page."
    ],
    "answer": "A,B",
    "title": "Question 112",
    "explanation": "Creating a custom object to store a copy of the record when changed and creating a custom object to store the previous and new field values are two possible actions that an architect can perform to meet the requirement of tracking field history on 100 Opportunity fields. A custom object can store more fields and records than the standard field history tracking feature, which has a limit of 20 fields per object and 18 or 24 months of data retention. A custom object can also be used for reporting and analysis of field history data. The other options are not feasible or effective for meeting the requirement"
  },
  {
    "content": "Universal Containers (UC) is building a Service Cloud call center application and has a multisystem support solution. UC would like or ensure that all systems have access to the same customer information.\nWhat solution should a data architect recommend?",
    "options": [
      "A. Make Salesforce the system of record for all data.",
      "B. Implement a master data management (MDM) strategy for customer data.",
      "C. Load customer data in all systems.",
      "D. Let each system be an owner of data it generates."
    ],
    "answer": "B",
    "title": "Question 113",
    "explanation": "A master data management (MDM) strategy for customer data can help UC ensure that all systems have access to the same customer information, without loading or duplicating data in all systems. An MDM strategy can also help UC avoid data conflicts and inconsistencies that may arise from having multiple systems as owners of data."
  },
  {
    "content": "Universal Containers has implemented Salesforce for its operations. In order for customers to be created in their MDM solution, the customer record needs to have the following attributes:\n1. First Name\n2. Last Name\n3. Email\nWhich option should the data architect recommend to mandate this when customers are created in Salesforce?",
    "options": [
      "A. Configure Page Layout marking attributes as required fields.",
      "B. Create validation rules to check If the required attributes are entered.",
      "C. Mark Fields for the attributes as required under Setup.",
      "D. Build validation in Integration with MDM to check required attributes."
    ],
    "answer": "B",
    "title": "Question 114",
    "explanation": "Creating validation rules to check if the required attributes are entered is the best option to mandate this when customers are created in Salesforce. Validation rules allow you to specify criteria that must be met before a record can be saved. You can use validation rules to ensure that customers have a first name, last name, and email when they are created in Salesforce. This way, you can prevent incomplete or invalid data from being sent to your MDM solution."
  },
  {
    "content": "Universal Containers has two systems. Salesforce and an on -premise ERP system. An architect has been tasked with copying Opportunity records to the ERP once they reach a Closed/Won Stage. The Opportunity record in the ERP system will be read-only for all fields copied in from Salesforce. What is the optimal real-time approach that achieves this solution?",
    "options": [
      "A. Implement a Master Data Management system to determine system of record.",
      "B. Implement a workflow rule that sends Opportunity data through Outbound Messaging.",
      "C. Have the ERP poll Salesforce nightly and bring in the desired Opportunities.",
      "D. Implement an hourly integration to send Salesforce Opportunities to the ERP system."
    ],
    "answer": "B",
    "title": "Question 115",
    "explanation": "Implementing a workflow rule that sends Opportunity data through Outbound Messaging is the optimal real-time approach that achieves the solution of copying Opportunity records to the ERP once they reach a Closed/Won Stage. A workflow rule can trigger an Outbound Message when an Opportunity record meets certain criteria, such as having a Closed/Won Stage. An Outbound Message can send data from Salesforce to an external system via SOAP API, without requiring any code. The external system can then process the data and create a read-only record in the ERP system. The other options are not optimal or real-time, as they would either require additional systems or tools, not provide real-time data synchronization, or not meet the frequency requirement"
  },
  {
    "content": "Universal Containers (UC) has several custom Visualforce applications have been developed in which users are able to edit Opportunity records. UC struggles with data completeness on their Opportunity records and has decided to make certain fields required that have not been in the past. The newly required fields are dependent on the Stage of the Opportunity, such that certain fields are only required once an Opportunity advances to later stages. There are two fields. What is the simplest approach to handle this new requirement?",
    "options": [
      "A. Update the Opportunity page layout to mark these fields as required.",
      "B. Use a validation rule for each field that takes the Stage into consideration.",
      "C. Update these Opportunity field definitions in Setup to be required.",
      "D. Write an Apex trigger that checks each field when records are saved."
    ],
    "answer": "B",
    "title": "Question 116",
    "explanation": "Using a validation rule for each field that takes the Stage into consideration is the simplest approach to handle this new requirement. A validation rule can enforce the field requirements based on the logic and criteria that you define, and display an error message when users try to save a record that does not meet the requirements. Updating the Opportunity page layout to mark these fields as required will not work because page layouts do not support conditional field requirements. Updating these Opportunity field definitions in Setup to be required will not work because it will apply to all stages and records. Writing an Apex trigger that checks each field when records are saved is not the simplest approach because it requires coding and testing"
  },
  {
    "content": "NTO uses salesforce to manage relationships and track sales opportunities. It has 10 million customers and 100 million opportunities. The CEO has been complaining 10 minutes to run and sometimes failed to load, throwing a time out error.\nWhich 3 options should help improve the dashboard performance?\nChoose 3 answers:",
    "options": [
      "A. Use selective queries to reduce the amount of data being returned.",
      "B. De-normalize the data by reducing the number of joins.",
      "C. Remove widgets from the dashboard to reduce the number of graphics loaded.",
      "D. Run the dashboard for CEO and send it via email.",
      "E. Reduce the amount of data queried by archiving unused opportunity records."
    ],
    "answer": "A,B,E",
    "title": "Question 117",
    "explanation": "To improve the dashboard performance, the data architect should use selective queries to reduce the amount of data being returned, de-normalize the data by reducing the number of joins, and reduce the amount of data queried by archiving unused opportunity records. These options will help optimize the query performance, reduce the query complexity, and free up storage space. Option C is incorrect because removing widgets from the dashboard to reduce the number of graphics loaded will not affect the dashboard performance significantly, and may reduce the usability and functionality of the dashboard. Option D is incorrect because running the dashboard for CEO and sending it via email will not improve the dashboard performance, but rather shift the burden to another user."
  }
]
